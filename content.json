{"pages":[{"title":"404 Not Found","text":"","link":"/404.html"}],"posts":[{"title":"58同城的技术架构","text":"单机系统Windows全家桶 Windows + IIS + SQL Server + C-Sharp —— 面向问题编程 建站之初，站点流量非常小，可能低于十万级别。这意味着，平均每秒钟也就几次访问。请求量比较低，数据量比较小，代码量也比较小，几个工程师，很短的时间搭起这样的系统，甚至没有考虑“架构”的问题。 和许多创业公司初期一样，最初58同城的站点架构特点是“ALL-IN-ONE”： 这是一个单机系统，所有的站点、数据库、文件都部署在一台服务器上。工程师每天的核心工作是CURD，浏览器端传过来一些数据，解析GET/POST/COOKIE中传过来的数据，拼装成一些CURD的sql语句访问数据库，数据库返回数据，拼装成页面，返回浏览器。相信很多创业团队的工程师，初期做的也是类似的工作。 58同城最初选择的是微软技术体系这条路：Windows、iis、SQL-Sever、C# LAMP如果重新再来，我们可能会选择LAMP（Linux + Apache + MySQL + PHP）体系。 为什么选择LAMP？ LAMP无须编译，发布快速，功能强大，社区活跃，从前端+后端+数据库访问+业务逻辑处理全部可以搞定，并且开源免费，公司做大了也不会有人上门收钱（不少公司吃过亏）。现在大家如果再创业，强烈建议使用LAMP。 —— 面向问题编程 ORM, 更好地CRUD初创阶段，工程师面临的主要问题：写CURD的sql语句很容易出错。 我们在这个阶段引进DAO（Data Access Object）和ORM（Object Relational Mapping ），让工程师们不再直接面对CURD的sql语句，而是面对他们比较擅长的面向对象开发，极大的提高了编码效率，降低了出错率。 分布式系统随着流量越来越大，老板不只要求“有一个可以看见的站点”，他希望网站能够正常访问，当然速度快点就更好了。 而此时系统面临问题是：流量的高峰期容易宕机，大量的请求会压到数据库上，数据库成为新的瓶颈，人多并行访问时站点非常卡。这时，我们的机器数量也从一台变成了多台，我们的系统成了所谓的（伪）“分布式架构”： 再见，Windows全家桶当流量达到百万甚至千万时，站点面临一个很大的问题就是性能和成本的折衷。上文提到58同城最初的技术选型是Windows，我们在这个阶段做了一次脱胎换骨的技术转型，全面转向开源技术： （1）操作系统转型Linux （2）数据库转型Mysql （3）web服务器转型Tomcat （4）开发语言转向了Java 其实，很多互联网公司在流量从小到大的过程中都经历过类似的转型，例如京东和淘宝。 反向代理为了保证站点的高可用，我们使用了反向代理。 什么是代理？代理就是代表用户访问xxoo站点。 什么是反向代理？反向代理代表的是58网站，用户不用关注访问是58同城的哪台服务器，由反向代理来代表58同城。58同城通过反向代理，DNS轮询， LVS等技术，来保证接入层的高可用性。 —— 面向问题编程 动静分离动态的页面通过Web-Server访问，静态的文件例如图片就放到单独的文件服务器上； Nginx中配置若是js,css,img等静态文件请求，则取文件返回；否则转发给Tomcat处理。 —— 面向问题编程 CDN加速对静态资源我们使用了CDN服务，用户就近访问，静态资源的访问速度得到很明显的提升； 可能的实现：在CDN厂商那里配置将自己域名CNAME到CDN域名，回源设置Nginx ip. 一文了解CDN加速原理 —— 面向问题编程 读写分离将落到数据库上的读写请求分派到不同的数据库服务器上； 互联网绝大部分的业务场景，都是读多写少。对58同城来说，绝大部分用户的需求是访问信息，搜索信息，只有少数的用户发贴。此时读取性能容易成为瓶颈，那么如何扩展整个站点架构的读性能呢？常用的方法是主从同步，增加从库。我们原来只有一个读数据库，现在有多个读数据库，就提高了读性能。 分布式一致性问题 在通过将数据复制到多个节点提高读性能和可用性（容忍节点故障）的同时，也带来了分布式一致性的问题。 —— 面向问题编程 数据库做了主从同步和读写分离之后，读写库之间数据的同步有一个延时，数据库数据量越大，从库越多时，延时越明显。对应到业务，有用户发帖子，马上去搜索可能搜索不到（着急的用户会再次发布相同的帖子） 我们在数据库层面也进行了垂直拆分，将单库数据量降下来，让读写延时得到缓解。 按业务拆分服务 站点耦合问题：对58同城而言，典型业务场景是：类别聚合的主页，发布信息的发布页，信息聚合的列表页，帖子内容的详细页，原来这些系统都耦合在一个站点中，出现问题的时候，整个系统都会受到影响。 要解决耦合的问题，最先想到的是针对核心业务做切分，工程师根据业务切分对系统也进行切分：我们将业务垂直拆分成了首页、发布页、列表页和详情页。 缓存随着用户量的增加，对站点可用性要求也越来越高，机器数也从最开始的几台上升到几百台。那么如何提供保证整个系统的可用性呢？首先，我们在业务层做了进一步的垂直拆分，同时引入了Cache，如下图所示： RPC在架构上，我们抽象了一个相对独立的服务层，所有数据的访问都通过这个服务层统一来管理，上游业务线就像调用本地函数一样，通过RPC的框架来调用这个服务获取数据，服务层对上游屏蔽底层数据库与缓存的复杂性。 参考链接 58同城架构演进 lvs为何不能完全替代DNS轮询","link":"/pages/58-arch/"},{"title":"大数据技术综述","text":"本文为对2014年的大数据技术综述论文《Big Data: A Survey》的翻译，论文作者为 Min Chen、Shiwen Mao和Yunhao Liu 英文版论文地址 Big Data: A Survey. 摘要在本文中，我们回顾了大数据的背景和最新技术。 我们首先介绍了大数据的一般背景，并回顾了相关技术， 如云计算、物联网、数据中心和Hadoop。 然后我们聚焦于大数据价值链的四个阶段，即数据生成、数据获取、数据存储和数据分析。 对于每个阶段，我们都会介绍一般背景、讨论技术挑战并回顾最新进展。 最后，我们研究了大数据的几个代表性应用， 包括企业管理、物联网、在线社交网络、医疗应用、集体智能和智能电网。 这些讨论旨在为读者提供这个令人兴奋的领域的一个全面的和宏观的概述。 本次综述以对开放问题和未来方向的讨论结束。 背景大数据时代的黎明在过去的20年里，各个领域的数据都有了大规模的增长。根据国际数据公司（International Data Corporation, IDC）的报告， 2011年全球创建和复制的数据总量为1.8ZB（约为$10^{21}$字节），在五年内增长了近9倍1。 未来，这个数字至少每两年就会翻一番。 在全球数据的爆炸式增长的背景下，大数据一词主要用于描述庞大的数据集。与传统数据集相比，大数据通常包含大量需要实时处理的非结构化数据。 大数据给予了我们发现数据背后隐藏价值的机遇，也带来了新的挑战，比如，如何高效的组织和管理大规模的数据。 最近，行业对大数据的巨大潜力产生了兴趣，许多政府机构宣布了加快大数据研究和应用的重大计划2。 此外，大数据问题经常被 公共媒体报道，如经济学人3,4、纽约时报5国家公共广播电台6,7。两大顶级科学期刊 Nature和Science也开设了专栏，讨论大数据的挑战和影响8,9。毋庸置疑，大数据时代已经来临。 如今，与互联网公司服务相关的大数据增长迅速。例如，谷歌处理数百个PB的数据， Facebook每月产生超过10pb的日志数据，百度处理数十个PB的数据， 阿里巴巴旗下的淘宝每天产生数十个TB的在线交易数据。图1展示了全球数据量的增长。 大型数据集的数量急剧增加的同时，也带来了许多具有亟待解决的富有挑战性的问题: 信息技术 (IT) 的最新进展使生成数据变得更加容易。 例如，平均每分钟有 72 小时的视频上传到 YouTube11。 因此，我们面临的主要挑战之一是从广泛分布的数据源收集和整合海量数据。 云计算和物联网的快速发展进一步推动了数据的急剧增长。云计算为数据资产提供了保障、访问场所和渠道。 在物联网的范式中，世界各地的传感器都在收集和传输数据，并在云中存储和处理。这样的数据无论在数量上还是相互关系上都将远远超过现有企业的IT架构和基础设施的能力， 日益增长的数据带来的另一个问题是，在硬件和软件基础设施有限的情况下，如何存储和管理如此庞大的异构数据集。 考虑到大数据的异构性、可扩展性、实时性、复杂性和私密性等特点，在分析、建模、可视化和预测等过程中， 需要对不同层次的数据集进行有效的“挖掘”，以揭示其内在属性，提高决策能力。 大数据的定义和特点大数据是一个抽象的概念。 除了意味着海量数据之外，它还有一些其他的特性，决定了它与“海量数据”或“超大数据”的区别。 目前，虽然大数据的重要性已得到普遍认可，但人们对其定义仍有不同看法。 一般来说，大数据是指传统的IT和软/硬件工具无法在可容忍的时间内感知、获取、管理和处理的数据集。 由于关注点不同，科技企业、研究学者、数据分析师、技术从业者对大数据的定义不同。 以下定义可以帮助我们更好地理解大数据的深刻社会、经济和技术内涵。 在2010年，Apache Hadoop将大数据定义为“在可接受的范围内，一般计算机无法捕获、管理和处理的数据集”。基于这一定义，2011年5月，全球咨询机构麦肯锡公司(McKinsey &amp; Company)宣布，大数据将成为创新、竞争和生产力的下一个前沿领域。大数据是指传统数据库软件无法获取、存储和管理的数据集。这一定义包括两个内涵:第一，符合大数据标准的数据集的容量在变化，可能会随着时间或技术的进步而增长;其次，在不同的应用中，符合大数据标准的数据量是不同的。目前，大数据一般在几TB到几PB10之间。从麦肯锡公司的定义可以看出，数据集的大小并不是衡量大数据的唯一标准。日益增长的数据规模和传统数据库技术无法处理的数据管理是接下来的两个关键特征。 事实上，早在2001年，人们就对大数据进行了定义。META(现为Gartner)的分析师Doug Laney在一份研究报告12中，用3Vs模型定义了数据增加带来的挑战和机遇，即Volume、Velocity和Variety的增加。虽然这个模型最初并没有被用来定义大数据，但是Gartner等许多企业，包括IBM13和微软14的一些研究部门，在接下来的十年15中仍然使用“3v”模型来描述大数据。在“3v”模型中，Volume意味着随着海量数据的产生和收集，数据规模越来越大;速度是指大数据的及时性，数据采集、分析等工作必须快速、及时地进行，才能最大限度地利用大数据的商业价值;多样性是指各种类型的数据，包括半结构化和非结构化数据，如音频、视频、网页、文本等，也包括传统的结构化数据。 然而，也有人持不同意见，其中包括IDC，大数据及其研究领域最具影响力的领导者之一。2011年，IDC的一份报告将大数据定义为:“大数据技术描述了新一代的技术和架构，旨在通过高速捕获、发现和/或分析，从各种各样的大量数据中经济地提取价值。”IBM1这个定义,大数据的特点可以概括为四个Vs。、Volume(大容量),Variety(各种形式)、Velocity(快速生成)和Value(价值巨大但密度低),如图2所示。这样的4v定义被广泛认可，因为它强调了大数据的意义和必要性，即挖掘隐藏的巨大价值。这一定义指出了大数据中最关键的问题，即如何从规模巨大、类型多样、生成迅速的数据集中发现价值。正如Facebook的副总工程师Jay Parikh所说:“如果你不利用收集到的数据，你只能拥有一堆数据，而不是大数据11。” 此外，NIST将大数据定义为“大数据是指数据量、采集速度或数据表示限制了使用传统关系方法进行有效分析的能力的数据，或可以通过重要的水平缩放技术有效处理的数据”。它专注于大数据的技术方面。这表明需要开发有效的方法或技术来分析和处理大数据。 业界和学术界对大数据的定义都有相当多的讨论16,17。大数据研究除了要有一个正确的定义外，还应该关注如何提取其价值，如何使用数据，如何将“一堆数据”转化为“大数据”。 大数据的价值麦肯锡公司在深入研究美国医疗保健、欧盟公共部门管理、美国零售、全球制造业和全球个人位置数据后，观察了大数据如何创造价值。麦肯锡报告通过对代表全球经济的五个核心产业的研究指出，大数据可以充分发挥经济功能，提高企业和公共部门的生产力和竞争力，为消费者创造巨大利益。在[10]中，麦肯锡总结了大数据可以创造的价值：如果能够创造性地、有效地利用大数据来提高效率和质量，美国医疗行业通过数据获得的潜在价值可能超过 3000 亿美元，从而减少美国医疗保健支出超过 8%；充分利用大数据的零售商可以将利润提高60%以上；大数据还可用于提高政府运营效率，欧洲发达经济体可节省超过 1000 亿欧元（不包括减少欺诈、错误和税收差异的影响）。 麦肯锡的报告被认为具有前瞻性和预测性，而以下事实可能验证大数据的价值。 2009年流感大流行期间，谷歌通过分析大数据及时获取信息，提供的信息甚至比疾病预防中心提供的信息更有价值。几乎所有国家都要求医院向疾病预防中心等机构通报新型流感病例。然而，患者在被感染后通常不会立即就医。从医院向疾控中心发送信息，由疾控中心对这些信息进行分析和总结，也需要一些时间。因此，当公众意识到新型流感大流行时，该疾病可能已经传播了一到两周，具有滞后性。谷歌发现，在流感传播过程中，其搜索引擎中频繁搜索的词条会与平时有所不同，词条的使用频率与流感传播的时间和地点都存在相关性。谷歌找到了 45 个与流感爆发密切相关的搜索词组，并将它们整合到特定的数学模型中，以预测流感的传播，甚至预测流感的传播地点。相关研究成果已发表在 Nature18 上。 2008年，微软收购了美国科技风险投资企业Farecast。Farecast拥有能够预测机票价格趋势和涨跌幅度的机票预测系统。该系统已被纳入微软的必应搜索引擎。到2012年，该系统为每位乘客节省了近50美元的票价，预测准确率高达75%。 目前，数据已成为一种可以与物质资产和人力资本相媲美的重要生产要素。随着多媒体、社交媒体、物联网的发展，企业收集的信息将越来越多，数据量将呈指数级增长。大数据在为企业和消费者创造价值方面具有巨大且不断增长的潜力。 大数据的发展1970年代后期，出现了“数据库机”的概念，这是一种专门用于存储和分析数据的技术。 随着数据量的增加，单个大型计算机系统的存储和处理能力变得不足。 1980年代，人们提出了“share nothing”，一种并行数据库系统，以满足日益增长的数据量的需求19。 无共享系统架构基于集群的使用，每台机器都有自己的处理器、存储和磁盘。 Teradata 系统是第一个成功的商业并行数据库系统。 这种数据库最近变得非常流行。 1986 年 6 月 2 日，Teradata 向 Kmart 交付了第一个存储容量为 1TB 的并行数据库系统，以帮助北美大型零售公司扩展其数据仓库20，这是一个里程碑式的事件。 1990年代后期，并行数据库的优势在数据库领域得到广泛认可。 然而，大数据也面临着许多挑战。随着互联网服务的发展，要索引和查询的内容迅速增长。因此，搜索引擎公司不得不面对处理这样的大数据的挑战。谷歌创建了GFS21和MapReduce22编程模型，以应对互联网规模下数据管理和分析带来的挑战。此外，用户、传感器等无处不在的数据源生成的内容也对势不可挡的数据流产生了冲击，这就要求对计算架构和大规模数据处理机制进行根本性的改变。2007年1月，数据库软件的先驱Jim Gray将这种转变称为“第四范式”23。他还认为，应对这种范式的唯一方法是开发新一代的计算工具来管理、可视化和分析大量数据。2011年6月，另一个里程碑事件发生了;EMC/IDC发表了《从混沌的[1]中提取价值》的研究报告，首次介绍了大数据的概念和潜力。该研究报告引发了业界和学术界对大数据的极大兴趣。 近年来，EMC、Oracle、IBM、微软、谷歌、亚马逊、Facebook等大公司几乎都启动了大数据项目。以IBM为例，自2005年以来，IBM共投资160亿美元进行了30次与大数据相关的收购。在学术界，大数据也受到了关注。2008年，《自然》杂志出版了大数据专刊。2011年，《科学》杂志还推出了关于大数据“数据处理”关键技术的专刊。2012年，欧洲信息学与数学研究联盟(ERCIM)新闻出版了大数据专刊。2012年初，在瑞士达沃斯论坛上，一份题为《大数据，大影响》的报告宣布，大数据已经成为一种新的经济资产，就像货币或黄金一样。国际研究机构Gartner在2012年至2013年发布了《Hype Cycles》，将大数据计算、社会分析、存储数据分析等分类为48个最值得关注的新兴技术。 美国等许多国家政府也非常重视大数据。 2012 年 3 月，奥巴马政府宣布投资 2 亿美元启动“大数据研发计划”，这是继 1993 年“信息高速公路”计划之后的第二个重大科技发展计划。2012 年 7 月， 日本总务省发布的“大力发展ICT日本”项目表明，大数据发展应成为国家战略，应用技术应成为重点。 2012年7月，联合国发布大数据促发展报告，总结了各国政府如何利用大数据更好地服务和保护人民。 大数据的挑战大数据时代急剧增加的数据洪流给数据的获取、存储、管理和分析带来了巨大的挑战。传统的数据管理和分析系统是基于关系数据库管理系统（RDBMS）。但是，此类 RDBMS 仅适用于结构化数据，而非半结构化或非结构化数据。此外，RDBMS 越来越多地使用越来越昂贵的硬件。显然，传统的 RDBMS 无法处理海量和异构的大数据。研究界从不同的角度提出了一些解决方案。例如，利用云计算来满足大数据对基础设施的要求，如成本效率、弹性、平滑升级/降级等。对于大规模无序数据集的永久存储和管理解决方案，分布式文件系统 [24] 和 NoSQL [25] 数据库是不错的选择。这样的编程框架在处理集群任务方面取得了巨大的成功，特别是在网页排名方面。基于这些创新技术或平台，可以开发出各种大数据应用。此外，部署大数据分析系统并非易事。 一些文献 [26-28] 讨论了大数据应用程序开发中的障碍。 主要挑战如下： 数据表示：许多数据集在类型、结构、语义、组织、粒度和可访问性方面都有一定程度的异构性。数据表示旨在使数据对计算机分析和用户解释更有意义。不恰当的数据表达会降低原始数据的价值，甚至可能阻碍有效的数据分析。高效的数据表示应反映数据的结构、类别和类型，以及集成的技术，以便对不同的数据集进行高效的操作。 减少冗余和数据压缩：通常，数据集中存在高度冗余。 不影响数据潜在价值的前提下减少冗余和压缩数据，可以有效降低整个系统的间接成本。 例如，传感器网络生成的大多数数据都是高度冗余的，可以成数量级地过滤和压缩。 数据生命周期管理：与存储系统相对缓慢的进步相比，无处不在的传感和计算正在以前所未有的速度和规模生成数据。 我们面临着许多紧迫的挑战，其中之一就是当前的存储系统无法支持如此海量的数据。 一般来说，大数据中隐藏的价值取决于数据的新鲜度。 因此，应制定与分析值相关的数据重要性原则，以决定哪些数据应存储，哪些数据应丢弃。 分析机制：大数据分析系统要在有限的时间内处理海量异构数据。 然而，传统的RDBMS设计严谨，缺乏可扩展性，无法满足性能要求。 非关系型数据库在处理非结构化数据方面表现出了独特的优势，开始成为大数据分析的主流。 尽管如此，非关系型数据库在性能和特定应用方面仍然存在一些问题。 我们将在 RDBMS 和非关系数据库之间找到一个折中的解决方案。 例如，一些企业采用了混合数据库架构，融合了两种数据库（如 Facebook 和淘宝）的优点。 数据保密性：目前大部分大数据服务商或拥有者由于能力有限，无法有效维护和分析如此庞大的数据集。 他们必须依靠专业人员或工具来分析这些数据，这增加了潜在的安全风险。 例如，交易数据集通常包括一组完整的运营数据，以驱动关键业务流程。 此类数据包含最低粒度的详细信息和一些敏感信息，例如信用卡号。 因此，只有在采取适当的预防措施保护此类敏感数据，以确保其安全时，才能将大数据的分析交付给第三方进行处理。 能源管理：大型机计算系统的能源消耗从经济和环境的角度都备受关注。 随着数据量和分析需求的增加，大数据的处理、存储和传输必然会消耗越来越多的电能。 因此，在保证可扩展性和可访问性的同时，需要建立大数据系统级的功耗控制和管理机制。 可扩展性：大数据分析系统必须支持当前和未来的数据集。 分析算法必须能够处理日益扩展和更复杂的数据集。 合作：大数据分析是一项跨学科研究，需要不同领域的专家合作，挖掘大数据的潜力。 必须建立一个全面的大数据网络架构，帮助各个领域的科学家和工程师访问不同类型的数据，充分利用他们的专业知识，协同完成分析目标。 相关技术为了深入了解大数据，本节将介绍与大数据密切相关的几项基础技术，包括云计算、物联网、数据中心和Hadoop。 大数据与云计算的关系云计算与大数据密切相关。云计算的关键组成部分如图3所示。大数据是计算密集型操作的对象，强调云系统的存储容量。云计算的主要目标是集中管理使用庞大的计算和存储资源，为大数据应用提供细粒度的计算能力。云计算的发展为大数据的存储和处理提供了解决方案。另一方面，大数据的出现也加速了云计算的发展。基于云计算的分布式存储技术可以有效地管理大数据;云计算的并行计算能力可以提高大数据采集和分析的效率。 尽管在云计算和大数据领域有很多重叠的技术，但它们在以下两个方面有所不同。首先，概念在一定程度上是不同的。云计算改变了IT架构，而大数据影响了业务决策。而大数据的顺利运行有赖于云计算作为基础设施。 第二，大数据和云计算的目标客户不同。云计算是一种针对首席信息官(CIO)的技术和产品，是一种先进的IT解决方案。大数据是一种针对首席执行官(CEO)的产品，专注于商业运营。由于决策者可能直接感受到来自市场竞争的压力，他们必须以更有竞争力的方式击败商业对手。随着大数据和云计算的发展，这两种技术必然会越来越多地交织在一起。云计算的功能与计算机和操作系统类似，提供系统级资源;大数据在云计算支持下的上层运行，提供类似数据库的功能和高效的数据处理能力。EMC总裁基辛格表示，大数据的应用必须基于云计算。 应用需求的快速增长和由虚拟化技术发展而来的云计算推动了大数据的演变。因此，云计算不仅为大数据提供计算和处理，它本身也是一种服务模式。在一定程度上，云计算的进步也促进了大数据的发展，两者相辅相成。 大数据与物联网的关系在物联网范式中，大量的网络传感器被嵌入到现实世界中的各种设备和机器中。这种传感器部署在不同的领域，可以收集各种数据，如环境数据、地理数据、天文数据和物流数据。移动设备、交通设施、公共设施、家电等都可以是物联网中的数据采集设备，如图4所示。 物联网产生的大数据由于采集的数据类型不同，与普通大数据相比具有不同的特征，其中最经典的特征包括异质性、多样性、非结构化、噪声和高冗余。虽然目前物联网数据还不是大数据的主导部分，但据惠普预测，到2030年，传感器数量将达到一万亿，物联网数据将成为大数据中最重要的部分。英特尔公司的一份报告指出，物联网大数据有三个符合大数据范式的特征:(1)丰富的终端产生大量数据;(ii)物联网产生的数据通常是半结构化或非结构化的;(iii)物联网数据只有经过分析才有用。 目前，物联网的数据处理能力已经落后于采集的数据，加快大数据技术的引入，促进物联网的发展刻不容缓。许多物联网运营商都意识到大数据的重要性，因为物联网的成功取决于大数据与云计算的有效整合。物联网的广泛应用也将使许多城市进入大数据时代。 在物联网应用中采用大数据的需求迫切，而大数据的发展已经落后。人们普遍认为，物联网和大数据是相互依存、共同发展的:一方面，物联网的广泛部署推动了数据在数量和类别上的高速增长，为大数据的应用和发展提供了机遇;另一方面，大数据技术在物联网中的应用也加速了物联网的研究进展和商业模式。 数据中心在大数据范式中，数据中心不仅是集中存储数据的平台，还承担着获取数据、管理数据、组织数据、利用数据价值和功能等更多的职责。数据中心主要关注的是“数据”而不是“中心”。它拥有大量的数据，并根据其核心目标和发展路径对数据进行组织和管理，这比拥有一个好的站点和资源更有价值。大数据的出现给数据中心带来了良好的发展机遇，同时也带来了巨大的挑战。大数据是一个新兴的范式，它将推动数据中心的基础设施和相关软件的爆炸式增长。物理数据中心网络是支持大数据的核心，也是目前[29]最迫切需要的关键基础设施。 大数据需要数据中心提供强大的后台支持。大数据范式对存储容量、处理容量以及网络传输容量都有更严格的要求。企业必须考虑到数据中心的发展，以提高在有限的性价比下快速有效的处理大数据的能力。数据中心需要为基础设施提供大量节点，构建高速的内部网络，有效散热，有效备份数据。只有建立一个高效节能、稳定、安全、可扩展、冗余的数据中心，才能保证大数据应用的正常运行。 大数据应用的增长加速了数据中心的革命和创新。许多大数据应用已经形成了自己独特的体系结构，直接推动了与数据中心相关的存储、网络和计算技术的发展。随着结构化和非结构化数据量的不断增长，分析数据来源的多样化，数据中心的数据处理和计算能力需要大幅提升。此外，随着数据中心规模的日益扩大，如何降低数据中心发展的运营成本也是一个重要的问题。 大数据赋予数据中心更多的功能。在大数据范式下，数据中心不仅要关注硬件设施，还要加强软能力，即大数据的获取、处理、组织、分析和应用的能力。数据中心可以帮助业务人员分析现有数据，发现业务运行中的问题，并从大数据中制定解决方案。 大数据与hadoop的关系目前Hadoop被广泛应用于行业中的大数据应用，如垃圾邮件过滤、网络搜索、点击流分析、社交推荐等。此外，现在有相当多的学术研究基于Hadoop。下面给出一些具有代表性的案例。正如2012年6月宣布的那样，雅虎在四个数据中心的42,000台服务器上运行Hadoop，以支持其产品和服务，如搜索和垃圾邮件过滤等。目前最大的Hadoop集群有4000个节点，但随着Hadoop 2.0的发布，节点数量将增加到10000个。同月，Facebook宣布他们的Hadoop集群可以处理100个PB数据，比2012年11月每天增加0.5个PB。[30]中列出了一些使用Hadoop进行分布式计算的知名机构。此外，许多公司提供Hadoop商业执行和/或支持，包括Cloudera、IBM、MapR、EMC和Oracle 在现代工业机械和系统中，传感器被广泛应用于收集信息，用于环境监测、故障预测等。Bahga和[31]中的其他人提出了一个用于数据组织和云计算基础设施的框架，称为CloudView。CloudView使用混合架构、本地节点和基于Hadoop的远程集群来分析机器生成的数据。局部节点用于实时故障预测;基于Hadoop的集群用于复杂的离线分析，例如案例驱动的数据分析. 基因组数据的指数增长和测序成本的急剧下降将生物科学和生物医学转变为数据驱动的科学。在[32]中，Gunarathne等人利用云计算基础设施、Amazon AWS、Microsoft Azune，以及基于MapReduce、Hadoop和Microsoft DryadLINQ的数据处理框架，运行了两个并行的生物医药应用:(i)基因组片段的组装;(ii)化学结构分析的降维。在随后的应用程序中，166-D数据集包括2600万个数据点。作者在效率、成本和可用性方面比较了所有框架的性能。通过研究，作者得出结论，松耦合将越来越多地应用于电子云的研究，并行编程技术(MapReduce)框架可以为用户提供更方便的服务界面，减少不必要的成本。 大数据的生成和获取大数据的存储数据的爆炸式增长对存储和管理提出了更加严格的要求。在本节中，我们将重点讨论大数据的存储。大数据存储是指对大规模数据集进行存储和管理，同时实现数据访问的可靠性和可用性。我们将回顾包括海量存储系统、分布式存储系统和大数据存储机制在内的重要问题。一方面，存储基础设施需要为信息存储服务提供可靠的存储空间;另一方面，它必须为海量数据的查询和分析提供强大的访问接口。 传统上，数据存储设备作为服务器的辅助设备，用于存储、管理、查找和分析结构化RDBMS中的数据。 随着数据的急剧增长，数据存储设备变得越来越重要，许多互联网公司追求大容量的存储以具有竞争力。 因此，迫切需要对数据存储进行研究。 海量数据存储系统为了满足海量数据的需求，各种存储系统应运而生。现有的海量存储技术可分为DAS (Direct Attached storage)和网络存储，网络存储又可分为NAS (network Attached storage)和SAN (storage Area network)。。 在DAS中，各种硬盘与服务器直接相连，数据管理以服务器为中心，存储设备为外围设备，每个设备占用一定的I/O资源，由单个应用软件进行管理。因此，DAS只适用于对接规模较小的服务器。但由于DAS的可扩展性较低，在增加存储容量时，其效率并不理想，即极大地限制了其可升级性和可扩展性。因此，DAS主要用于个人电脑和小型服务器 网络存储就是利用网络为用户提供数据访问和共享的联合接口。网络存储设备包括专用的数据交换设备、磁盘阵列、tap库等存储介质以及专用的存储软件。具有较强的可扩展性。 NAS实际上是网络的辅助存储设备。它通过集线器或交换机通过TCP/IP协议直接连接到网络。在NAS中，数据以文件的形式传输。与DAS相比，NAS服务器通过网络间接访问存储设备，大大减少了I/O负担。 NAS是面向网络的，而SAN是专门为可伸缩和带宽密集型网络的数据存储而设计的，例如，具有光纤连接的高速网络。在SAN中，数据存储管理是在存储局域网内相对独立的，它利用内部任意节点之间基于多路径的数据交换，实现最大程度的数据共享和数据管理。 从数据存储系统的组织结构上看，DAS、NAS和SAN均可分为三部分:(1)磁盘阵列:磁盘阵列是存储系统的基础，是数据存储的根本保障;(ii)提供一个或多个磁盘阵列和服务器之间连接的连接和网络子系统;(iii)存储管理软件，处理多台服务器的数据共享、容灾等存储管理任务。。 分布式存储系统与CAP大数据带来的第一个挑战是如何开发一个大规模的分布式存储系统来高效地处理和分析数据。使用分布式系统存储海量数据，需要考虑以下因素: 一致性（Consistency）:分布式存储系统需要多台服务器协同存储数据。服务器越多，服务器故障的概率就越大。通常数据被分成多个部分存储在不同的服务器上，以确保在服务器故障时的可用性。但是，服务器故障和并行存储可能会导致相同数据的不同副本之间不一致。一致性是指确保相同数据的多个副本是相同的。 可用性（Availability）:分布式存储系统运行在多台服务器上。随着使用更多的服务器，服务器故障是不可避免的。如果整个系统不受严重影响，就可以满足客户的读写要求。这个属性称为可用性。 分区容错（Partition Tolerance）：分布式存储系统中的多个服务器通过网络连接。 网络可能存在链路/节点故障或临时拥塞。 分布式系统应该对网络故障引起的问题有一定的容忍度。 当网络被分区时，我们希望分布式存储仍然可以很好地工作。 Eric Brewer在2000年提出了CAP[80,81]理论，指出分布式系统不能同时满足一致性、可用性和分区容错的要求;三个要求中最多可以同时满足两个。麻省理工学院的Seth Gilbert和Nancy Lynch在2002年证明了CAP理论的正确性。由于一致性、可用性和分区容错不能同时实现，我们可以根据不同的设计目标，通过忽略分区容错来拥有CA系统，通过忽略可用性来拥有CP系统，以及忽略一致性的AP系统。下面将对这三个系统进行讨论。 CA系统没有分区容错，也就是说，它们不能处理网络故障。因此，CA系统通常被视为只有一台服务器的存储系统，例如传统的小型关系数据库。这样的系统以数据的单副本为特征，这样很容易保证一致性。良好的关系数据库设计保证了可用性。然而，由于CA系统不能处理网络故障，因此它们不能扩展到使用许多服务器。因此，大多数大型存储系统都是CP系统和AP系统。 与CA系统相比，CP系统保证了分区的容错。因此，CP系统可以扩展为分布式系统。CP系统通常维护相同数据的多个副本，以确保一定程度的容错。CP系统还确保数据一致性，即保证相同数据的多个副本完全相同。然而，CP不能确保良好的可用性，因为保证一致性的成本很高。因此，CP系统适用于负载适中但对数据准确性要求严格的场景(如交易数据)。BigTable和Hbase是两种流行的CP系统。 AP 系统也确保分区容错。 AP 系统与 CP 系统的不同之处在于，AP 系统确保可用性。 同时，AP系统只保证最终一致性，而不是前两个系统的强一致性。 因此，AP系统只适用于请求频繁但对准确性要求不高的场景。 例如，在在线社交网络服务（SNS）系统中，对数据的并发访问很多，但一定量的数据错误是可以容忍的。 此外，由于AP系统确保最终的一致性，在一定的延迟后仍然可以获得准确的数据。 因此，AP系统也可以在没有严格实时要求的情况下使用。 Dynamo 和 Cassandra 是两个流行的 AP 系统。 大数据的存储机制大量关于大数据的研究促进了大数据存储机制的发展。现有的大数据存储机制可分为自底向上的三个层次:文件系统、数据库和编程模型。 文件系统是上层应用程序的基础。谷歌的GFS是一个可扩展的分布式文件系统，支持大规模、分布式、数据密集型应用程序[25]。GFS使用廉价的商品服务器实现容错，并为客户提供高性能服务。GFS支持读比写更频繁的大型文件应用程序。但是，GFS也有一些限制，比如单点故障和小文件的性能差。GFS的后继者Colossus[82]克服了这些限制。 此外，其他公司和研究人员也有自己的解决方案，以满足不同的大数据存储需求。例如，HDFS和Kosmosfs都是GFS的开源代码的衍生品。微软开发Cosmos[83]是为了支持其搜索和广告业务。Facebook利用Haystack[84]来存储大量的小尺寸照片。淘宝还开发了TFS和FastDFS。综上所述，经过多年的开发和业务运营，分布式文件系统已经相对成熟。因此，在本节的其余部分中，我们将重点关注其他两个层次。 数据库技术数据库技术已经发展了30多年。各种数据库系统被开发来处理不同规模的数据集，并支持各种应用程序。传统的关系数据库已经不能满足大数据在类别和规模上的挑战。NoSQL数据库(即非传统的关系数据库)在大数据存储中越来越流行。NoSQL数据库具有灵活的模式、支持简单易用的复制、具有简单的API、最终一致性和支持大容量数据等特点。NoSQL数据库正在成为大数据的核心技术。在本节中，我们将研究以下三种主要的NoSQL数据库:键值数据库、面向列的数据库和面向文档的数据库，每一种都基于特定的数据模型。 键值数据库：键值数据库由简单的数据模型构成，数据对应键值存储。 每个键都是唯一的，客户可以根据键输入查询值。 此类数据库结构简单，比关系数据库相比，现代键值数据库具有高扩展性和查询响应时间短的特点。 在过去的几年里，在亚马逊的 Dynamo 系统 [85] 的启发下，键值数据库层出不穷。 我们将介绍 Dynamo 和其他几个具有代表性的键值数据库。 Dynamo：Dynamo 是一个高可用、可扩展的分布式键值数据存储系统。用于存储和管理亚马逊电商平台中一些核心服务的状态，可以通过键访问实现。关系型数据库的公共模式可能会产生无效数据，限制数据规模和可用性，而Dynamo可以通过简单的key-object接口解决这些问题，由简单的读写操作构成。 Dynamo 通过数据分区、数据复制和对象编辑机制实现弹性和可用性。 Dynamo 分区计划依赖于一致性哈希[86]，其主要优点是节点通过仅影响直接相邻的节点，不影响其他节点，为多个主存储机器分配负载。 Dynamo 将数据复制到 N 组服务器，其中 N 为可配置参数，以实现高可用性和持久性。 Dynamo 系统还提供最终一致性，以便对所有副本进行异步更新。 Voldemort：Voldemort 也是一个键值存储系统，最初是为 LinkedIn 开发的，现在仍在使用。 Voldemort中的键和值是由表格和图像构成的复合对象。 Voldemort界面包括三个简单的操作：读取、写入和删除，所有这些操作都是通过键来确认的。 Voldemort提供多个版本的异步更新并发控制，但不保证数据的一致性。 但是，Voldemort 支持乐观锁定以实现一致的多记录更新。 当更新操作与其他操作发生冲突时，更新操作将退出。 Voldmort 的数据复制机制与 Dynamo 相同。 Voldemort 不仅将数据存储在 RAM 中，还允许将数据插入存储引擎。 特别是，Voldemort 支持两种存储引擎，包括 Berkeley DB 和 Random Access Files。 键值数据库出现在几年前。深受Amazon Dynamo DB影响的其他key - value存储系统包括Redis、Tokyo Canbinet和Tokyo Tyrant、Memcached和Memcache DB、Riak和Scalaris，它们都通过将键分布到节点上提供了可扩展性。Voldemort、Riak、Tokyo Cabinet和Memecached可以利用附加存储设备在RAM或磁盘中存储数据。其他存储系统在RAM中存储数据并提供磁盘备份，或者依靠复制和恢复来避免备份。 面向列的数据库:面向列的数据库根据列而不是行来存储和处理数据。将列和行分割到多个节点中，以实现可扩展性。面向列的数据库主要是受到谷歌的BigTable的启发。在本节中，我们首先讨论BigTable，然后介绍几个衍生工具。 BigTable: BigTable是一种分布式、结构化的数据存储系统，用于处理数千台商业服务器之间的大规模(PB类)数据[87]。Bigtable的基本数据结构是一个具有稀疏、分布式和持久存储的多维序列映射。映射的索引是行键、列键和时间戳，映射中的每个值都是一个未分析的字节数组。BigTable中的每个行键是一个64KB的字符串。通过字典顺序，行被存储并连续地分割成片(tablet,即分布单元)，以实现负载平衡。因此，读取一小行数据是非常有效的，因为它只涉及到与一小部分机器的通信。列根据键的前缀分组，从而形成列族。这些列族是访问控制的基本单元。时间戳是64位整数，用于区分单元格值的不同版本。客户端可以灵活地确定存储的单元版本的数量。这些版本按时间戳降序排列，因此总是读取最新的版本。 BigTable API的特点是可以创建和删除tablet和列族，以及修改集群、表和列族的元数据。客户端应用程序可以插入或删除BigTable的值，从列中查询值，或浏览表中的子数据集。Bigtable还支持其他一些特征，比如单行事务处理。用户可以利用这些特性进行更复杂的数据处理。 BigTable 执行的每个过程都包括三个主要组件：主服务器、平板服务器和客户端库。 Bigtable 只允许分发一套Master server，负责为Tablet server 分配tablet，检测添加或删除的Tablet server，进行负载均衡。 此外，它还可以修改 BigTable Schema，例如创建表和列族，以及收集 GFS 中保存的垃圾以及删除或禁用的文件，并在特定的 BigTable 实例中使用它们。 每个 tablet server 管理一个 Tablet set，负责对加载的 Tablet 进行读写。 当 Tablets 太大时，它们会被服务器分段。 应用程序客户端库用于与 BigTable 实例进行通信。 BigTable 基于 Google 的许多基本组件，包括 GFS [25]、集群管理系统、SSTable 文件格式和 Chubby [88]。 GFS 用于存储数据和日志文件。 集群管理系统负责任务调度、资源共享、机器故障处理、机器状态监控。 SSTable 文件格式用于在内部存储 BigTable 数据，它提供持久的、有序的和不可更改的键和值之间的映射作为任何字节字符串。 BigTable 在服务器中利用 Chubby 完成以下任务： 1) 确保任何时候最多有一个活动的 Master 副本； 2）存储BigTable数据的bootstrap位置； 3）查找平板服务器； 4) Table server出现故障时进行错误恢复； 5) 存储 BigTable 架构信息； 6) 存储访问控制表。 传统数据分析大数据分析方法大数据分析的架构大数据挖掘和分析工具大数据的应用结论、有待解决的问题和展望参考文献见原论文。 持续更新中...","link":"/pages/big-data-survey/"},{"title":"斗鱼的技术架构","text":"业务后台架构早期架构 (2014) 单体应用 Nginx+PHP+Memcached+MySQL 问题：粉丝收到大主播开播信息后，涌入直播间，获取直播间信息等。Memcache采用一致性哈希，同一个直播间的缓存key落在了同一个Memcachd节点，会造成改节点负载过高。 另外，如果Memcached此时没有该直播间信息的缓存，此时大量的并发请求会到达MySQL(缓存击穿) 2016年 业务垂直拆分，更好地故障隔离 缓存：Memcahced换成了主从结构的Redis，全量的直播间数据缓存到Redis。多从库，每个都有大主播的数据信息，分担大主播流量，避免出现单节点性能问题。（分布式复制(replicas)）。 问题1：内网带宽过高，因为Memcached在响应客户端时，会做数据压缩。Redis没有。内网带宽涨了4，5倍。 问题2：redis没做业务隔离和限流，大量业务使用同一组Redis。某个业务Redis使用不当，造成redis阻塞，导致业务全部瘫痪。 微服务化 gRPC PHP换成Go，服务端做内存缓存池和Redis连接池优化。 服务注册和发现基于etcd RPC客户端调服务端时的负载均衡策略：与服务端负载相关的p2c算法。 部分监控数据采用prometheus存储 音视频架构待更新。 参考链接 斗鱼：如何打造一个高性能、高可用直播系统架构","link":"/pages/douyu-arch/"},{"title":"不知道做啥？看下那些真实业务场景下的技术问题","text":"(云计算) 如何解决Serverless函数冷启动的问题？ (智能运维) 如何对业务指标和基础监控指标做异常检测？","link":"/pages/problems/"},{"title":"(智能运维) 如何对监控指标做异常检测？","text":"背景参考链接 腾讯 织云Metis时间序列异常检测全方位解析 美团 外卖订单量预测异常报警模型实践 滴滴 滴滴出行海量数据场景下的智能监控与故障定位实践 有空就更新...","link":"/pages/ts-anomaly-detection/"},{"title":"(云计算) 如何解决Serverless函数冷启动的问题？","text":"背景 如果你的PaaS能够有效地在20毫秒内启动实例并运行半秒,那么就可以称之为Serverless。 —— AWS云架构战略副总裁Adrian Cockcroft1 Serverless的最大卖点之一就是在做弹性伸缩时可以将实例数缩到0，没流量时不花钱，有流量来时再扩容。然而理想很丰满，现实很骨感。 无论是在流量刚到达时实例数从0到1，还是为了处理更多的流量将实例数从m扩到n的过程，不得不面对的一个问题就是：冷启动。冷启动时间太长， 就会出现请求超时。 何为冷启动 冷启动是指在函数调用链路中包含了资源调度、镜像/代码下载、启动容器、运行时初始化、用户代码初始化等环节。当冷启动完成后，函数实例就绪， 后续请求就能直接被函数执行2。 传统的优化方案尽量避免冷启动 某个实例处理完请求后，不会因为后面2，3分钟没流量就被销毁。各个厂商都会让其继续存在一段时间，比如30分钟。 预热 Amazon EventBridge。搞个定时任务定时去触发Lambda实例，比如每5分钟触发一次3。 “这样在真正想要处理的事件抵达之前，就会有已经被预热 Lambda 保持激活状态以等待响应”。这种方法也可以用来处理在出现时间上具有规律的流量高峰。 预置并发。”预置并发是 AWS Lambda 在 2019 年推出的一个功能，该功能能够指定处于激活状态的 Lambda 实例数量，使函数保持初始化状态”。 在k8s就是相当于给HPA设定个最小副本数呗3。 除了浪费资源，上面的策略也无法解决突发流量场景下从m扩到n时的冷启动问题。 调度 （腾讯）降低调度复杂度4。 原有的调度模块需要考虑的维度很多： 数十种虚拟机的配置、CPU/内存/存储/网络等亲和性和反亲和性、 部署组的需求-跨宿主机，交换机，机架、资源利用率、碎片填充...... 轻量化调度模块：少数的虚拟机配置、宿主机的可用资源离线计算 容器启动 （腾讯）轻量级虚拟机4。 网络 （腾讯）SCF网络4 镜像/代码下载 （平台侧）代码缓存4。 （用户侧）既然代码下载耗时，那你尽可能减小代码包的大小吧。比如在程序中移除不必要的代码、减少不必要的第三方库依赖等。 运行时初始化 如果是用Java, 可以禁掉JVM的分层编译以降低运行速度的代价来换取启动速度的提升3。 其它优化方案WebAssembly WebAssembly 与 Kubernetes 双剑合璧：机遇与挑战 云原生的 WebAssembly 能取代 Docker 吗？ IsolatesCloudFlare的方案 Cloud Computing without Containers 其Serverless平台Workers已宣称零冷启动时间。 Say goodbye to cold starts—support for 0ms worldwide 各个厂商的冷启动时延? 参考链接 AWS 从IaaS到FaaS—— Serverless架构的前世今生 美团 Serverless平台Nest的探索与实践 亚马逊 降低AWS Lambda 冷启动时间的4种方案 腾讯云 Go FaaSter ：Serverless平台冷启动优化 持续更新中...","link":"/pages/serverless-coldstart/"},{"title":"Twitter的技术架构","text":"背景2013年时推文的TPS2013年8月3日星期六，人们观看了《天空之城》的播出，Twiiter当时推文的TPS峰值达到了：每秒 143,199 条推文 通常Twiiter每天接收超过 5 亿条推文，这意味着平均每秒大约 5,700 条推文。这个特殊的峰值大约是平时的 25 倍。 在此高峰期间，用户在 Twitter 上没有遇到任何问题。这是Twiter的目标之一：确保无论世界各地发生什么事情，Twitter 始终可用。 这个目标在三年前感觉遥不可及，当时 在2010 年世界杯中, 射门、点球和黄牌或红牌 等带来的突增流量导致 Twitter 在短时间内无法使用。 2010年 Web框架 单体（不是单机）Ruby on Rails应用 (不了解Ruby on Rails? 你可以将它想象成Python中的Django) 当时Twiiter可以说是跑着世界上最大的Ruby on Rails系统， 有大约200个工程师参与其中。 这个系统是单体的，我们所做的一切，从管理原始数据库和 memcache 连接到渲染站点和呈现公共 API，都在一个代码库中。这使得工程师越来越难以把握这堆东西是如何组织在一起的，而且这使得工程团队难以并行进行开发工作。代码库变得越来越难以维护，团队不断地花时间进行“考古挖掘”以了解某些功能. 存储单Master MySQL数据库, 按时间维度进行分片（Sharding）。 缓存Redis和memcached 2013年单体应用到面向服务 从我们的单体 Ruby 应用程序转移到更面向服务的应用程序。我们首先专注于创建推文、时间线和用户服务——我们的“核心名词”。 面向服务的架构允许我们并行开发系统——我们就RPC接口达成一致，然后独立实现各自的接口。这也意味着每个系统的逻辑系统自成一体。如果我们需要更改有关推文的某些内容，我们可以在一个位置进行更改，即推文服务。 RPC框架Twiiter的RPC框架为Finagle。关于对其的介绍，可以看这篇文章Netty at Twitter with Finagle， 扩展 Twitter的一部分是从单一的 Ruby on Rails 应用程序转变为面向服务的架构。为了构建这个新架构，我们需要一个高性能、容错、协议无关的异步 RPC 框架。在面向服务的架构中，服务大部分时间都在等待其他上游服务的响应。使用异步库允许服务同时处理请求并充分利用硬件。虽然 Finagle 可以直接构建在 NIO 之上，但 Netty 已经解决了我们可能会遇到的许多问题，并提供了干净清晰的 API。 特点 基于Netty 支持服务发现，负载均衡，失败重试，连接池，数据统计和分布式链路追踪等 负责分布式链路追踪的是Zipkin 负载均衡算法：Deterministic Aperture: A distributed, load balancing algorithm 存储 即使我们将单体应用程序分解为服务，存储仍然是一个巨大瓶颈。当时，Twitter 将推文存储在单个主 MySQL 数据库中。我们采取了按时间存储数据的策略——数据库中的每一行都是一条推文，我们将推文按顺序存储在数据库中，当数据库满时，我们启动另一个并重新配置软件以写入另一个数据库。这种策略为我们赢得了一些时间，但是，我们仍然在写入大量推文时遇到问题，因为它们都将被序列化到单个主库中。 Gizzard用Gizzard来创建分布式的数据库分片。 对一个推文写入请求，Gizzard对其做一个哈希（可以自定义哈希算法），选择一个数据库并插入。之前由于是单Master MySQL, 可以依靠MySQL来产生一个唯一的id，而现在得采用另外一种方式。Twiiter使用Snowflake才创建唯一id。 Gizzard对数据使用了复制和分区。 数据密集型应用系统设计(DDIA)中对复制和分区有如下介绍。 数据分布在多个节点上有两种常见的方式： 复制（Replication） ​ 在几个不同的节点上保存数据的相同副本，可能放在不同的位置。 复制提供了冗余：如果一些节点不可用，剩余的节点仍然可以提供数据服务。 复制也有助于改善性能。 分区 (Partitioning) ​ 将一个大型数据库拆分成较小的子集（称为分区（partitions）），从而不同的分区可以指派给不同的节点（node）（亦称分片（shard）） 复制和分区是不同的机制，但它们经常同时使用。 其它 搜索：Java via Lucene 社交图谱 FlockDB 大数据分析（比如海量日志）：Hadoop with Scalding 计算机集群管理：mesos Mesos Architecture 持续更新中... 参考链接 2009 Scaling Twitter: Making Twitter 10000 Percent Faster 2013 Evolution of The Twitter Stack 2013 New Tweets per second record, and how! 2013 The Architecture Twitter Uses To Deal With 150M Active Users, 300K QPS, A 22 MB/S Firehose, And Send Tweets In Under 5 Seconds 2017 The Infrastructure Behind Twitter: Scale Twitter的RPC框架 Netty at Twitter with Finagle 2019 Twitter 宣布抛弃 Mesos，全面转向 Kubernetes","link":"/pages/twitter-arch/"},{"title":"微博的技术架构","text":"背景 每条微博，在技术上也被称为status或feed. 微博中的关注页是一个feed流，聚合了自己关注了的人的微博。 Feed服务架构 主体采用拉模式, 部分高级功能（？）采用推模式。 早期存储用的MySQL, MySQL和Web服务之间加一层Memcached缓存。 MySQL优化： 复制：多slave读。主从同步问题，确保最终一致性。 分区：按年月分表。为了支持分页，得做一个索引，既统计每个用户在每个月发了多少个微博。不然你不能迅速知道用户查看第N页时，到底在哪张表。 参考链接 新浪微博Feed服务架构 亿级用户下的新浪微博平台架构 杨卫华：新浪微博的架构发展历程 微博 CacheService 架构浅析 微博cache设计谈 突发热点场景下 - 微博高可用注册中心 vintage 设计 &amp; 实践","link":"/pages/weibo-arch/"},{"title":"一文了解CDN加速原理","text":"CDN（Content Delivery Network，内容分发网络）是构建在现有互联网基础之上的一层智能虚拟网络，通过在网络各处部署节点服务器，实现将源站内容分发至所有CDN节点，使用户可以就近获得所需的内容。CDN服务缩短了用户查看内容的访问延迟，提高了用户访问网站的响应速度与网站的可用性，解决了网络带宽小、用户访问量大、网点分布不均等问题。 CDN加速原理当用户访问使用CDN服务的网站时，本地DNS服务器通过CNAME方式将最终域名请求重定向到CDN服务。CDN通过一组预先定义好的策略(如内容类型、地理区域、网络负载状况等)，将当时能够最快响应用户的CDN节点IP地址提供给用户，使用户可以以最快的速度获得网站内容。使用CDN后的HTTP请求处理流程如下： CDN节点有缓存场景 HTTP请求流程说明： 1、用户在浏览器输入要访问的网站域名，向本地DNS发起域名解析请求。 2、域名解析的请求被发往网站授权DNS服务器。 3、网站DNS服务器解析发现域名已经CNAME到了www.example.com.c.cdnhwc1.com。 4、请求被指向CDN服务。 5、CDN对域名进行智能解析，将响应速度最快的CDN节点IP地址返回给本地DNS。 6、用户获取响应速度最快的CDN节点IP地址。 7、浏览器在得到速度最快节点的IP地址以后，向CDN节点发出访问请求。 8、CDN节点将用户所需资源返回给用户。 CDN节点无缓存场景 HTTP请求流程说明： 1、用户在浏览器输入要访问的网站域名，向本地DNS发起域名解析请求。 2、域名解析的请求被发往网站授权DNS服务器。 3、网站DNS服务器解析发现域名已经CNAME到了www.example.com.c.cdnhwc1.com。 4、请求被指向CDN服务。 5、CDN对域名进行智能解析，将响应速度最快的CDN节点IP地址返回给本地DNS。 6、用户获取响应速度最快的CDN节点IP地址。 7、浏览器在得到速度最快节点的IP地址以后，向CDN节点发出访问请求。 8、CDN节点回源站拉取用户所需资源。 9、将回源拉取的资源缓存至节点。 10、将用户所需资源返回给用户。 域名加速配置根据上面的原理，要做域名加速，就得使用阿里云/腾讯云等云厂商的cdn服务，配置的基本流程就是： 输入你要加速的域名www.abc.com, 它返回一个域名给你，比如www.abc.com.cdn.dnsv1.com。 配置源站地址：可以是ip地址形式或域名形式。CDN节点不包含请求内容时将访问该地址(域名)获取请求内容 缓存规则配置：比如配置缓存过期策略，不缓存动态文件(比如.php)等。默认的缓存过期策略通常与http头的Cache-Control， max-age等字段相关，可参考云厂商的官方文档。 去域名控制台配置域名解析，将www.abc.com CNAME到 abc.com.cdn.dnsv1.com 参考链接 CDN的加速原理是什么？ 腾讯云域名加速配置 阿里云域名加速配置 腾讯云CDN节点缓存过期配置","link":"/pages/wiki-cdn/"},{"title":"知乎的技术架构","text":"Timeline架构演进这里的Timeline是指按时间排列的用户的动态（关注了问题，赞同了回答，发表回答等）， 分为： 某个人动态列表： https://www.zhihu.com/people/yhz731137336/activities —— 被查看的用户，所有动作以时间序排列在 feed 流里 实现比较简单，只要存一个列表，响应请求翻页就好了； 我的关注列表：https://www.zhihu.com/follow —— 我关注的所有人的所有动作以时间序排列在 feed 流里 需要把所有好友的动态合并成一个列表，然后对这个列表进行翻页。 “推”模实现: 既类似于个人动态页为每个用户的关注页存储一张列表（类似一个收件箱），关注的人产生动态后，将动态追加到列表中 浪费资源：A关注了1万个活跃用户，需要为他频繁地追加列表，占用过多存储。而A可能已经很久没有知乎了。或者A是个大V，有10万个粉丝，每产生一个动态，就得推到10万个粉丝。（这种问题其实可以通过只推给在线用户解决） 可编辑性较弱：用户 A 取消关注了用户 B，需要在 A 的列表中删除 B 的动态；用户 A 增加关注了用户 C，需要在 A 的列表中加入 C 的动态，而这种操作的复杂度是恐怖的 知乎采用“拉”模式：根据动态发起者组织列表，当用户请求时，将所有的动作发起者动态 merge 在一起，实时生成 feed 列表。 旧架构 个人动态页数据用HBase存储。用”用户id+创建时间”表示一条动态。 “A点赞B”之类的操作消息会放进消息队列(Kafka), 然后消费，对Hbase进行CURD。可能会出现重复消费问题。比如收到多条A点赞B的消息。所以插入前要先去重，不然会出现重复的动态。 上面的架构图中，Hbase下面查询时还接了一层缓存，我们可以把这次查询的内容(以memberid, start_time, 取得的feed的max_time标识？这段时间某个动态删除了怎么处理cache?)缓存起来，可以降低Hbase查询压力以及更快地返回内容给用户。 关注列表A点赞了B回答，数据用redis的zset存储。key为A的id, item 为动作内容”vote_up_answer_b”，score为动作时间。 请求用户A的关注列表时，拿到被A关注的人最近的动态，按时间序merge在一起。 上面的feed的应该不包含feed的具体内容，那具体内容存储在哪里? zset 存储的信息 key: source_type,source_id value: item_type,item_id,item_action score: heat_score (当前源的热度分) 新Timeline架构 将Feed的历史操作信息存到了HBase，方便追溯和查问题 个人动态页最近两个月的动态请求，拉取redis的数据。旧数据请求才会拉取Hbase。 参考链接 知乎 timeline 演进之路 知乎首页 Feed 演进","link":"/pages/zhihu-arch/"}],"tags":[{"name":"大数据","slug":"大数据","link":"/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"智能运维","slug":"智能运维","link":"/tags/%E6%99%BA%E8%83%BD%E8%BF%90%E7%BB%B4/"},{"name":"Serverless","slug":"Serverless","link":"/tags/Serverless/"},{"name":"feed服务","slug":"feed服务","link":"/tags/feed%E6%9C%8D%E5%8A%A1/"}],"categories":[{"name":"技术架构","slug":"技术架构","link":"/categories/%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84/"},{"name":"综述","slug":"综述","link":"/categories/%E7%BB%BC%E8%BF%B0/"},{"name":"问题","slug":"问题","link":"/categories/%E9%97%AE%E9%A2%98/"},{"name":"一文了解","slug":"一文了解","link":"/categories/%E4%B8%80%E6%96%87%E4%BA%86%E8%A7%A3/"}]}