<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>一文了解 Linux 网络包发送过程 - 面向问题编程</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="面向问题编程"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="面向问题编程"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="大家好，我是飞哥!   半年前我以源码的方式描述了网络包的接收过程。之后不断有粉丝提醒我还没聊发送过程呢。好，安排！ 在开始今天的文章之前，我先来请大家思考几个小问题。  问1：我们在查看内核发送数据消耗的 CPU 时，是应该看 sy 还是 si ？ 问2：为什么你服务器上的 &amp;#x2F;proc&amp;#x2F;softirqs 里 NET_RX 要比 NET_TX 大的多的多？ 问3：发送网络数据的"><meta property="og:type" content="blog"><meta property="og:title" content="一文了解 Linux 网络包发送过程"><meta property="og:url" content="https://www.heapoverflow.cn/pages/wiki-linux-network/"><meta property="og:site_name" content="面向问题编程"><meta property="og:description" content="大家好，我是飞哥!   半年前我以源码的方式描述了网络包的接收过程。之后不断有粉丝提醒我还没聊发送过程呢。好，安排！ 在开始今天的文章之前，我先来请大家思考几个小问题。  问1：我们在查看内核发送数据消耗的 CPU 时，是应该看 sy 还是 si ？ 问2：为什么你服务器上的 &amp;#x2F;proc&amp;#x2F;softirqs 里 NET_RX 要比 NET_TX 大的多的多？ 问3：发送网络数据的"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.heapoverflow.cn/img/wiki-linux-network/24.jpg"><meta property="article:published_time" content="2022-03-27T04:23:35.230Z"><meta property="article:modified_time" content="2022-03-30T04:23:28.302Z"><meta property="article:author" content="面向问题编程"><meta property="article:tag" content="Linux网络"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/wiki-linux-network/24.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.heapoverflow.cn/pages/wiki-linux-network/"},"headline":"一文了解 Linux 网络包发送过程","image":["https://www.heapoverflow.cn/img/wiki-linux-network/24.jpg"],"datePublished":"2022-03-27T04:23:35.230Z","dateModified":"2022-03-30T04:23:28.302Z","author":{"@type":"Person","name":"面向问题编程"},"publisher":{"@type":"Organization","name":"面向问题编程","logo":{"@type":"ImageObject","url":"https://www.heapoverflow.cn/img/logo.svg"}},"description":"大家好，我是飞哥!   半年前我以源码的方式描述了网络包的接收过程。之后不断有粉丝提醒我还没聊发送过程呢。好，安排！ 在开始今天的文章之前，我先来请大家思考几个小问题。  问1：我们在查看内核发送数据消耗的 CPU 时，是应该看 sy 还是 si ？ 问2：为什么你服务器上的 &#x2F;proc&#x2F;softirqs 里 NET_RX 要比 NET_TX 大的多的多？ 问3：发送网络数据的"}</script><link rel="canonical" href="https://www.heapoverflow.cn/pages/wiki-linux-network/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=G-ETGL163DQH" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-ETGL163DQH');</script><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><script data-ad-client="ca-pub-6757924689439593" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }
          Array
              .from(document.querySelectorAll('.tab-content'))
              .forEach($tab => {
                  $tab.classList.add('is-hidden');
              });
          Array
              .from(document.querySelectorAll('.tabs li'))
              .forEach($tab => {
                  $tab.classList.remove('is-active');
              });
          const $activeTab = document.querySelector(location.hash);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
          const $tabMenu = document.querySelector(`a[href="${location.hash}"]`);
          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.0.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="面向问题编程" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/categories/%E9%97%AE%E9%A2%98/">问题</a><a class="navbar-item" href="/categories/%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84/">技术架构</a><a class="navbar-item" href="/categories/%E4%B8%80%E6%96%87%E4%BA%86%E8%A7%A3/">一文了解</a></div><div class="navbar-end"><a class="navbar-item" rel="noopener" title="分类" href="/categories">分类</a><a class="navbar-item" rel="noopener" title="标签" href="/tags">标签</a><a class="navbar-item" rel="noopener" title="归档" href="/archives">归档</a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="/img/wiki-linux-network/24.jpg" alt="一文了解 Linux 网络包发送过程"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-03-27T04:23:35.230Z" title="2022/3/27 下午12:23:35">2022-03-27</time>发表</span><span class="level-item"><time dateTime="2022-03-30T04:23:28.302Z" title="2022/3/30 下午12:23:28">2022-03-30</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E4%B8%80%E6%96%87%E4%BA%86%E8%A7%A3/">一文了解</a></span><span class="level-item">1 小时读完 (大约10256个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">一文了解 Linux 网络包发送过程</h1><div class="content"><p>大家好，我是飞哥!  </p>
<p>半年前我以源码的方式描述了网络包的接收过程。之后不断有粉丝提醒我还没聊发送过程呢。好，安排！</p>
<p>在开始今天的文章之前，我先来请大家思考几个小问题。</p>
<ul>
<li>问1：我们在查看内核发送数据消耗的 CPU 时，是应该看 sy 还是 si ？</li>
<li>问2：为什么你服务器上的 &#x2F;proc&#x2F;softirqs 里 NET_RX 要比 NET_TX 大的多的多？</li>
<li>问3：发送网络数据的时候都涉及到哪些内存拷贝操作？</li>
</ul>
<p>这些问题虽然在线上经常看到，但我们似乎很少去深究。如果真的能透彻地把这些问题理解到位，我们对性能的掌控能力将会变得更强。</p>
<span id="more"></span>

<p>带着这三个问题，我们开始今天对 Linux 内核网络发送过程的深度剖析。还是按照我们之前的传统，先从一段简单的代码作为切入。如下代码是一个典型服务器程序的典型的缩微代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">int main()&#123;  </span><br><span class="line"> fd = socket(AF_INET, SOCK_STREAM, 0);  </span><br><span class="line"> bind(fd, ...);  </span><br><span class="line"> listen(fd, ...);  </span><br><span class="line">  </span><br><span class="line"> cfd = accept(fd, ...);  </span><br><span class="line">  </span><br><span class="line"> // 接收用户请求  </span><br><span class="line"> read(cfd, ...);  </span><br><span class="line">  </span><br><span class="line"> // 用户请求处理  </span><br><span class="line"> dosometing();   </span><br><span class="line">  </span><br><span class="line"> // 给用户返回结果  </span><br><span class="line"> send(cfd, buf, sizeof(buf), 0);  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>


<p>今天我们来讨论上述代码中，调用 send 之后内核是怎么样把数据包发送出去的。本文基于Linux 3.10，网卡驱动采用Intel的igb网卡举例。</p>
<p><strong>预警：本文共有一万多字，25 张图，长文慎入！</strong></p>
<h2 id="一、Linux-网络发送过程总览"><a href="#一、Linux-网络发送过程总览" class="headerlink" title="一、Linux 网络发送过程总览"></a>一、Linux 网络发送过程总览</h2><p>我觉得看 Linux 源码最重要的是得有整体上的把握，而不是一开始就陷入各种细节。</p>
<p>我这里先给大家准备了一个总的流程图，简单阐述下 send 发送了的数据是如何一步一步被发送到网卡的。</p>
<p><img src="/img/wiki-linux-network/1.jpg"></p>
<p>在这幅图中，我们看到用户数据被拷贝到内核态，然后经过协议栈处理后进入到了 RingBuffer 中。随后网卡驱动真正将数据发送了出去。当发送完成的时候，是通过硬中断来通知 CPU，然后清理 RingBuffer。  </p>
<p>因为文章后面要进入源码，所以我们再从源码的角度给出一个流程图。</p>
<p><img src="/img/wiki-linux-network/2.jpg"></p>
<p>虽然数据这时已经发送完毕，但是其实还有一件重要的事情没有做，那就是释放缓存队列等内存。  </p>
<p>那内核是如何知道什么时候才能释放内存的呢，当然是等网络发送完毕之后。网卡在发送完毕的时候，会给 CPU 发送一个硬中断来通知 CPU。更完整的流程看图：</p>
<p><img src="/img/wiki-linux-network/3.jpg"></p>
<p>注意，我们今天的主题虽然是发送数据，但是硬中断最终触发的软中断却是 NET_RX_SOFTIRQ，而并不是 NET_TX_SOFTIRQ ！！！（T 是 transmit 的缩写，R 表示 receive）  </p>
<p><strong>意不意外，惊不惊喜？？？</strong></p>
<p>所以这就是开篇问题 1 的一部分的原因（注意，这只是一部分原因）。</p>
<blockquote>
<p>问1：在服务器上查看 &#x2F;proc&#x2F;softirqs，为什么 NET_RX 要比 NET_TX 大的多的多？</p>
</blockquote>
<p>传输完成最终会触发 NET_RX，而不是 NET_TX。所以自然你观测 &#x2F;proc&#x2F;softirqs 也就能看到 NET_RX 更多了。</p>
<p>好，现在你已经对内核是怎么发送网络包的有一个全局上的把握了。不要得意，我们需要了解的细节才是更有价值的地方，让我们继续！！</p>
<h2 id="二、网卡启动准备"><a href="#二、网卡启动准备" class="headerlink" title="二、网卡启动准备"></a>二、网卡启动准备</h2><p>现在的服务器上的网卡一般都是支持多队列的。每一个队列上都是由一个 RingBuffer 表示的，开启了多队列以后的的网卡就会对应有多个 RingBuffer。</p>
<p><img src="/img/wiki-linux-network/4.jpg"></p>
<p>网卡在启动时最重要的任务之一就是分配和初始化 RingBuffer，理解了 RingBuffer 将会非常有助于后面我们掌握发送。因为今天的主题是发送，所以就以传输队列为例，我们来看下网卡启动时分配 RingBuffer 的实际过程。  </p>
<p>在网卡启动的时候，会调用到 __igb_open 函数，RingBuffer 就是在这里分配的。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: drivers/net/ethernet/intel/igb/igb_main.c  </span></span><br><span class="line"><span class="type">static</span> <span class="type">int</span> __igb_open(<span class="keyword">struct</span> net_device *netdev, <span class="type">bool</span> resuming)  </span><br><span class="line">&#123;  </span><br><span class="line"> <span class="class"><span class="keyword">struct</span> <span class="title">igb_adapter</span> *<span class="title">adapter</span> =</span> netdev_priv(netdev);  </span><br><span class="line">  </span><br><span class="line"> <span class="comment">//分配传输描述符数组  </span></span><br><span class="line"> err = igb_setup_all_tx_resources(adapter);  </span><br><span class="line">  </span><br><span class="line"> <span class="comment">//分配接收描述符数组  </span></span><br><span class="line"> err = igb_setup_all_rx_resources(adapter);  </span><br><span class="line">  </span><br><span class="line"> <span class="comment">//开启全部队列  </span></span><br><span class="line"> netif_tx_start_all_queues(netdev);  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>


<p>在上面 __igb_open 函数调用 igb_setup_all_tx_resources 分配所有的传输 RingBuffer, 调用 igb_setup_all_rx_resources 创建所有的接收 RingBuffer。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: drivers/net/ethernet/intel/igb/igb_main.c  </span></span><br><span class="line"><span class="type">static</span> <span class="type">int</span> <span class="title function_">igb_setup_all_tx_resources</span><span class="params">(<span class="keyword">struct</span> igb_adapter *adapter)</span>  </span><br><span class="line">&#123;  </span><br><span class="line"> <span class="comment">//有几个队列就构造几个 RingBuffer  </span></span><br><span class="line"> <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; adapter-&gt;num_tx_queues; i++) &#123;  </span><br><span class="line">  igb_setup_tx_resources(adapter-&gt;tx_ring[i]);  </span><br><span class="line"> &#125;  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>


<p>真正的 RingBuffer 构造过程是在 igb_setup_tx_resources 中完成的。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: drivers/net/ethernet/intel/igb/igb_main.c  </span></span><br><span class="line"><span class="type">int</span> <span class="title function_">igb_setup_tx_resources</span><span class="params">(<span class="keyword">struct</span> igb_ring *tx_ring)</span>  </span><br><span class="line">&#123;  </span><br><span class="line"> <span class="comment">//1.申请 igb_tx_buffer 数组内存  </span></span><br><span class="line"> size = <span class="keyword">sizeof</span>(<span class="keyword">struct</span> igb_tx_buffer) * tx_ring-&gt;count;  </span><br><span class="line"> tx_ring-&gt;tx_buffer_info = vzalloc(size);  </span><br><span class="line">  </span><br><span class="line"> <span class="comment">//2.申请 e1000_adv_tx_desc DMA 数组内存  </span></span><br><span class="line"> tx_ring-&gt;size = tx_ring-&gt;count * <span class="keyword">sizeof</span>(<span class="keyword">union</span> e1000_adv_tx_desc);  </span><br><span class="line"> tx_ring-&gt;size = ALIGN(tx_ring-&gt;size, <span class="number">4096</span>);  </span><br><span class="line"> tx_ring-&gt;desc = dma_alloc_coherent(dev, tx_ring-&gt;size,  </span><br><span class="line">        &amp;tx_ring-&gt;dma, GFP_KERNEL);  </span><br><span class="line">  </span><br><span class="line"> <span class="comment">//3.初始化队列成员  </span></span><br><span class="line"> tx_ring-&gt;next_to_use = <span class="number">0</span>;  </span><br><span class="line"> tx_ring-&gt;next_to_clean = <span class="number">0</span>;  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>


<p>从上述源码可以看到，实际上一个 RingBuffer 的内部不仅仅是一个环形队列数组，而是有两个。</p>
<p>1）igb_tx_buffer 数组：这个数组是内核使用的，通过 vzalloc 申请的。<br>2）e1000_adv_tx_desc 数组：这个数组是网卡硬件使用的，硬件是可以通过 DMA 直接访问这块内存，通过 dma_alloc_coherent 分配。</p>
<p>这个时候它们之间还没有啥联系。将来在发送的时候，这两个环形数组中相同位置的指针将都将指向同一个 skb。这样，内核和硬件就能共同访问同样的数据了，内核往 skb 里写数据，网卡硬件负责发送。</p>
<p><img src="/img/wiki-linux-network/5.jpg"></p>
<p>最后调用 netif_tx_start_all_queues 开启队列。另外，对于硬中断的处理函数 igb_msix_ring 其实也是在 __igb_open 中注册的。  </p>
<h2 id="三、accept-创建新-socket"><a href="#三、accept-创建新-socket" class="headerlink" title="三、accept 创建新 socket"></a>三、accept 创建新 socket</h2><p>在发送数据之前，我们往往还需要一个已经建立好连接的 socket。</p>
<p>我们就以开篇服务器缩微源代码中提到的 accept 为例，当 accept 之后，进程会创建一个新的 socket 出来，然后把它放到当前进程的打开文件列表中，专门用于和对应的客户端通信。</p>
<p>假设服务器进程通过 accept 和客户端建立了两条连接，我们来简单看一下这两条连接和进程的关联关系。</p>
<p><img src="/img/wiki-linux-network/6.jpg"></p>
<p>其中代表一条连接的 socket 内核对象更为具体一点的结构图如下。  </p>
<p><img src="/img/wiki-linux-network/7.jpg"></p>
<p>为了避免喧宾夺主，accept 详细的源码过程这里就不介绍了，感兴趣请参考 <a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MjM5Njg5NDgwNA==&mid=2247484905&idx=1&sn=a74ed5d7551c4fb80a8abe057405ea5e&scene=21#wechat_redirect">《图解 | 深入揭秘 epoll 是如何实现 IO 多路复用的！》</a>。一文中的第一部分。  </p>
<p>今天我们还是把重点放到数据发送过程上。</p>
<h2 id="四、发送数据真正开始"><a href="#四、发送数据真正开始" class="headerlink" title="四、发送数据真正开始"></a>四、发送数据真正开始</h2><h3 id="4-1-send-系统调用实现"><a href="#4-1-send-系统调用实现" class="headerlink" title="4.1 send 系统调用实现"></a>4.1 send 系统调用实现</h3><p>send 系统调用的源码位于文件 net&#x2F;socket.c 中。在这个系统调用里，内部其实真正使用的是 sendto 系统调用。整个调用链条虽然不短，但其实主要只干了两件简单的事情，</p>
<ul>
<li>第一是在内核中把真正的 socket 找出来，在这个对象里记录着各种协议栈的函数地址。</li>
<li>第二是构造一个 struct msghdr 对象，把用户传入的数据，比如 buffer地址、数据长度啥的，统统都装进去.</li>
</ul>
<p>剩下的事情就交给下一层，协议栈里的函数 inet_sendmsg 了，其中 inet_sendmsg 函数的地址是通过 socket 内核对象里的 ops 成员找到的。大致流程如图。</p>
<p><img src="/img/wiki-linux-network/8.jpg"></p>
<p>有了上面的了解，我们再看起源码就要容易许多了。源码如下：  </p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: net/socket.c  </span></span><br><span class="line">SYSCALL_DEFINE4(send, <span class="type">int</span>, fd, <span class="type">void</span> __user *, buff, <span class="type">size_t</span>, len,  </span><br><span class="line">  <span class="type">unsigned</span> <span class="type">int</span>, flags)  </span><br><span class="line">&#123;  </span><br><span class="line"> <span class="keyword">return</span> sys_sendto(fd, buff, len, flags, <span class="literal">NULL</span>, <span class="number">0</span>);  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line">SYSCALL_DEFINE6(......)  </span><br><span class="line">&#123;  </span><br><span class="line"> <span class="comment">//1.根据 fd 查找到 socket  </span></span><br><span class="line"> sock = sockfd_lookup_light(fd, &amp;err, &amp;fput_needed);  </span><br><span class="line">  </span><br><span class="line"> <span class="comment">//2.构造 msghdr  </span></span><br><span class="line"> <span class="class"><span class="keyword">struct</span> <span class="title">msghdr</span> <span class="title">msg</span>;</span>  </span><br><span class="line"> <span class="class"><span class="keyword">struct</span> <span class="title">iovec</span> <span class="title">iov</span>;</span>  </span><br><span class="line">  </span><br><span class="line"> iov.iov_base = buff;  </span><br><span class="line"> iov.iov_len = len;  </span><br><span class="line"> msg.msg_iovlen = <span class="number">1</span>;  </span><br><span class="line">  </span><br><span class="line"> msg.msg_iov = &amp;iov;  </span><br><span class="line"> msg.msg_flags = flags;  </span><br><span class="line"> ......  </span><br><span class="line">  </span><br><span class="line"> <span class="comment">//3.发送数据  </span></span><br><span class="line"> sock_sendmsg(sock, &amp;msg, len);  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>


<p>从源码可以看到，我们在用户态使用的 send 函数和 sendto 函数其实都是 sendto 系统调用实现的。send 只是为了方便，封装出来的一个更易于调用的方式而已。</p>
<p>在 sendto 系统调用里，首先根据用户传进来的 socket 句柄号来查找真正的 socket 内核对象。接着把用户请求的 buff、len、flag 等参数都统统打包到一个 struct msghdr 对象中。</p>
<p>接着调用了 sock_sendmsg &#x3D;&gt; __sock_sendmsg &#x3D;&#x3D;&gt;  __sock_sendmsg_nosec。在__sock_sendmsg_nosec 中，调用将会由系统调用进入到协议栈，我们来看它的源码。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: net/socket.c  </span></span><br><span class="line"><span class="type">static</span> <span class="keyword">inline</span> <span class="type">int</span> __sock_sendmsg_nosec(...)  </span><br><span class="line">&#123;  </span><br><span class="line"> ......  </span><br><span class="line"> <span class="keyword">return</span> sock-&gt;ops-&gt;sendmsg(iocb, sock, msg, size);  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>


<p>通过第三节里的 socket 内核对象结构图，我们可以看到，这里调用的是 sock-&gt;ops-&gt;sendmsg 实际执行的是 inet_sendmsg。这个函数是 AF_INET 协议族提供的通用发送函数。</p>
<h3 id="4-2-传输层处理"><a href="#4-2-传输层处理" class="headerlink" title="4.2 传输层处理"></a>4.2 传输层处理</h3><h4 id="1）传输层拷贝"><a href="#1）传输层拷贝" class="headerlink" title="1）传输层拷贝"></a>1）传输层拷贝</h4><p>在进入到协议栈 inet_sendmsg 以后，内核接着会找到 socket 上的具体协议发送函数。对于 TCP 协议来说，那就是 tcp_sendmsg（同样也是通过 socket 内核对象找到的）。</p>
<p>在这个函数中，内核会申请一个内核态的 skb 内存，将用户待发送的数据拷贝进去。注意这个时候不一定会真正开始发送，如果没有达到发送条件的话很可能这次调用直接就返回了。大概过程如图：</p>
<p><img src="/img/wiki-linux-network/9.jpg"></p>
<p>我们来看 inet_sendmsg 函数的源码。  </p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: net/ipv4/af_inet.c  </span></span><br><span class="line"><span class="type">int</span> <span class="title function_">inet_sendmsg</span><span class="params">(......)</span>  </span><br><span class="line">&#123;  </span><br><span class="line"> ......  </span><br><span class="line"> <span class="keyword">return</span> sk-&gt;sk_prot-&gt;sendmsg(iocb, sk, msg, size);  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>


<p>在这个函数中会调用到具体协议的发送函数。同样参考第三节里的 socket 内核对象结构图，我们看到对于 TCP 协议下的 socket 来说，来说 sk-&gt;sk_prot-&gt;sendmsg 指向的是 tcp_sendmsg（对于 UPD 来说是 udp_sendmsg）。</p>
<p>tcp_sendmsg 这个函数比较长，我们分多次来看它。先看这一段</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: net/ipv4/tcp.c  </span></span><br><span class="line"><span class="type">int</span> <span class="title function_">tcp_sendmsg</span><span class="params">(...)</span>  </span><br><span class="line">&#123;  </span><br><span class="line"> <span class="keyword">while</span>(...)&#123;  </span><br><span class="line">  <span class="keyword">while</span>(...)&#123;  </span><br><span class="line">   <span class="comment">//获取发送队列  </span></span><br><span class="line">   skb = tcp_write_queue_tail(sk);  </span><br><span class="line">  </span><br><span class="line">   <span class="comment">//申请skb 并拷贝  </span></span><br><span class="line">   ......  </span><br><span class="line">  &#125;  </span><br><span class="line"> &#125;  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>


<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: include/net/tcp.h  </span></span><br><span class="line"><span class="type">static</span> <span class="keyword">inline</span> <span class="keyword">struct</span> sk_buff *<span class="title function_">tcp_write_queue_tail</span><span class="params">(<span class="type">const</span> <span class="keyword">struct</span> sock *sk)</span>  </span><br><span class="line">&#123;  </span><br><span class="line"> <span class="keyword">return</span> skb_peek_tail(&amp;sk-&gt;sk_write_queue);  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>


<p>理解对 socket 调用 tcp_write_queue_tail 是理解发送的前提。如上所示，这个函数是在获取 socket 发送队列中的最后一个 skb。skb 是 struct sk_buff 对象的简称，用户的发送队列就是该对象组成的一个链表。</p>
<p><img src="/img/wiki-linux-network/10.jpg"></p>
<p>我们再接着看 tcp_sendmsg 的其它部分。  </p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: net/ipv4/tcp.c  </span></span><br><span class="line"><span class="type">int</span> <span class="title function_">tcp_sendmsg</span><span class="params">(<span class="keyword">struct</span> kiocb *iocb, <span class="keyword">struct</span> sock *sk, <span class="keyword">struct</span> msghdr *msg,  </span></span><br><span class="line"><span class="params">  <span class="type">size_t</span> size)</span>  </span><br><span class="line">&#123;  </span><br><span class="line"> <span class="comment">//获取用户传递过来的数据和标志  </span></span><br><span class="line"> iov = msg-&gt;msg_iov; <span class="comment">//用户数据地址  </span></span><br><span class="line"> iovlen = msg-&gt;msg_iovlen; <span class="comment">//数据块数为1  </span></span><br><span class="line"> flags = msg-&gt;msg_flags; <span class="comment">//各种标志  </span></span><br><span class="line">  </span><br><span class="line"> <span class="comment">//遍历用户层的数据块  </span></span><br><span class="line"> <span class="keyword">while</span> (--iovlen &gt;= <span class="number">0</span>) &#123;  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">//待发送数据块的地址  </span></span><br><span class="line">  <span class="type">unsigned</span> <span class="type">char</span> __user *from = iov-&gt;iov_base;  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">while</span> (seglen &gt; <span class="number">0</span>) &#123;  </span><br><span class="line">  </span><br><span class="line">   <span class="comment">//需要申请新的 skb  </span></span><br><span class="line">   <span class="keyword">if</span> (copy &lt;= <span class="number">0</span>) &#123;  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">//申请 skb，并添加到发送队列的尾部  </span></span><br><span class="line">    skb = sk_stream_alloc_skb(sk,  </span><br><span class="line">         select_size(sk, sg),  </span><br><span class="line">         sk-&gt;sk_allocation);  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">//把 skb 挂到socket的发送队列上  </span></span><br><span class="line">    skb_entail(sk, skb);  </span><br><span class="line">   &#125;  </span><br><span class="line">  </span><br><span class="line">   <span class="comment">// skb 中有足够的空间  </span></span><br><span class="line">   <span class="keyword">if</span> (skb_availroom(skb) &gt; <span class="number">0</span>) &#123;  </span><br><span class="line">    <span class="comment">//拷贝用户空间的数据到内核空间，同时计算校验和  </span></span><br><span class="line">    <span class="comment">//from是用户空间的数据地址   </span></span><br><span class="line">    skb_add_data_nocache(sk, skb, from, copy);  </span><br><span class="line">   &#125;   </span><br><span class="line">   ......  </span><br></pre></td></tr></table></figure>


<p>这个函数比较长，不过其实逻辑并不复杂。其中 msg-&gt;msg_iov 存储的是用户态内存的要发送的数据的 buffer。接下来在内核态申请内核内存，比如 skb，并把用户内存里的数据拷贝到内核态内存中。<strong>这就会涉及到一次或者几次内存拷贝的开销</strong>。</p>
<p><img src="/img/wiki-linux-network/11.jpg"></p>
<p>至于内核什么时候真正把 skb 发送出去。在 tcp_sendmsg 中会进行一些判断。  </p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: net/ipv4/tcp.c  </span></span><br><span class="line"><span class="type">int</span> <span class="title function_">tcp_sendmsg</span><span class="params">(...)</span>  </span><br><span class="line">&#123;  </span><br><span class="line"> <span class="keyword">while</span>(...)&#123;  </span><br><span class="line">  <span class="keyword">while</span>(...)&#123;  </span><br><span class="line">   <span class="comment">//申请内核内存并进行拷贝  </span></span><br><span class="line">  </span><br><span class="line">   <span class="comment">//发送判断  </span></span><br><span class="line">   <span class="keyword">if</span> (forced_push(tp)) &#123;  </span><br><span class="line">    tcp_mark_push(tp, skb);  </span><br><span class="line">    __tcp_push_pending_frames(sk, mss_now, TCP_NAGLE_PUSH);  </span><br><span class="line">   &#125; <span class="keyword">else</span> <span class="keyword">if</span> (skb == tcp_send_head(sk))  </span><br><span class="line">    tcp_push_one(sk, mss_now);    </span><br><span class="line">   &#125;  </span><br><span class="line">   <span class="keyword">continue</span>;  </span><br><span class="line">  &#125;  </span><br><span class="line"> &#125;  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>


<p>只有满足 forced_push(tp) 或者 skb &#x3D;&#x3D; tcp_send_head(sk) 成立的时候，内核才会真正启动发送数据包。其中 forced_push(tp) 判断的是未发送的数据数据是否已经超过最大窗口的一半了。</p>
<p>条件都不满足的话，<strong>这次的用户要发送的数据只是拷贝到内核就算完事了！</strong></p>
<h4 id="2）传输层发送"><a href="#2）传输层发送" class="headerlink" title="2）传输层发送"></a>2）传输层发送</h4><p>假设现在内核发送条件已经满足了，我们再来跟踪一下实际的发送过程。对于上小节函数中，当满足真正发送条件的时候，无论调用的是 __tcp_push_pending_frames 还是 tcp_push_one 最终都实际会执行到 tcp_write_xmit。</p>
<p>所以我们直接从 tcp_write_xmit 看起，这个函数处理了传输层的拥塞控制、滑动窗口相关的工作。满足窗口要求的时候，设置一下 TCP 头然后将 skb 传到更低的网络层进行处理。</p>
<p><img src="/img/wiki-linux-network/12.jpg"></p>
<p>我们来看下 tcp_write_xmit 的源码。  </p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: net/ipv4/tcp_output.c  </span></span><br><span class="line"><span class="type">static</span> <span class="type">bool</span> <span class="title function_">tcp_write_xmit</span><span class="params">(<span class="keyword">struct</span> sock *sk, <span class="type">unsigned</span> <span class="type">int</span> mss_now, <span class="type">int</span> nonagle,  </span></span><br><span class="line"><span class="params">      <span class="type">int</span> push_one, <span class="type">gfp_t</span> gfp)</span>  </span><br><span class="line">&#123;  </span><br><span class="line"> <span class="comment">//循环获取待发送 skb  </span></span><br><span class="line"> <span class="keyword">while</span> ((skb = tcp_send_head(sk)))   </span><br><span class="line"> &#123;  </span><br><span class="line">  <span class="comment">//滑动窗口相关  </span></span><br><span class="line">  cwnd_quota = tcp_cwnd_test(tp, skb);  </span><br><span class="line">  tcp_snd_wnd_test(tp, skb, mss_now);  </span><br><span class="line">  tcp_mss_split_point(...);  </span><br><span class="line">  tso_fragment(sk, skb, ...);  </span><br><span class="line">  ......  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">//真正开启发送  </span></span><br><span class="line">  tcp_transmit_skb(sk, skb, <span class="number">1</span>, gfp);  </span><br><span class="line"> &#125;  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>


<p>可以看到我们之前在网络协议里学的滑动窗口、拥塞控制就是在这个函数中完成的，这部分就不过多展开了，感兴趣同学自己找这段源码来读。我们今天只看发送主过程，那就走到了 tcp_transmit_skb。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: net/ipv4/tcp_output.c  </span></span><br><span class="line"><span class="type">static</span> <span class="type">int</span> <span class="title function_">tcp_transmit_skb</span><span class="params">(<span class="keyword">struct</span> sock *sk, <span class="keyword">struct</span> sk_buff *skb, <span class="type">int</span> clone_it,  </span></span><br><span class="line"><span class="params">    <span class="type">gfp_t</span> gfp_mask)</span>  </span><br><span class="line">&#123;  </span><br><span class="line"> <span class="comment">//1.克隆新 skb 出来  </span></span><br><span class="line"> <span class="keyword">if</span> (likely(clone_it)) &#123;  </span><br><span class="line">  skb = skb_clone(skb, gfp_mask);  </span><br><span class="line">  ......  </span><br><span class="line"> &#125;  </span><br><span class="line">  </span><br><span class="line"> <span class="comment">//2.封装 TCP 头  </span></span><br><span class="line"> th = tcp_hdr(skb);  </span><br><span class="line"> th-&gt;source  = inet-&gt;inet_sport;  </span><br><span class="line"> th-&gt;dest  = inet-&gt;inet_dport;  </span><br><span class="line"> th-&gt;window  = ...;  </span><br><span class="line"> th-&gt;urg   = ...;  </span><br><span class="line"> ......  </span><br><span class="line">  </span><br><span class="line"> <span class="comment">//3.调用网络层发送接口  </span></span><br><span class="line"> err = icsk-&gt;icsk_af_ops-&gt;queue_xmit(skb, &amp;inet-&gt;cork.fl);  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>


<p>第一件事是先克隆一个新的 skb，这里重点说下为什么要复制一个 skb 出来呢？</p>
<p>是因为 skb 后续在调用网络层，最后到达网卡发送完成的时候，这个 skb 会被释放掉。而我们知道 TCP 协议是支持丢失重传的，在收到对方的 ACK 之前，这个 skb 不能被删除。所以内核的做法就是每次调用网卡发送的时候，实际上传递出去的是 skb 的一个拷贝。等收到 ACK 再真正删除。</p>
<p>第二件事是修改 skb 中的 TCP header，根据实际情况把 TCP 头设置好。这里要介绍一个小技巧，skb 内部其实包含了网络协议中所有的 header。在设置 TCP 头的时候，只是把指针指向 skb 的合适位置。后面再设置 IP 头的时候，在把指针挪一挪就行，避免频繁的内存申请和拷贝，效率很高。</p>
<p><img src="/img/wiki-linux-network/13.jpg"></p>
<p>tcp_transmit_skb 是发送数据位于传输层的最后一步，接下来就可以进入到网络层进行下一层的操作了。调用了网络层提供的发送接口icsk-&gt;icsk_af_ops-&gt;queue_xmit()。  </p>
<p>在下面的这个源码中，我们的知道了 queue_xmit 其实指向的是 ip_queue_xmit 函数。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: net/ipv4/tcp_ipv4.c  </span></span><br><span class="line"><span class="type">const</span> <span class="class"><span class="keyword">struct</span> <span class="title">inet_connection_sock_af_ops</span> <span class="title">ipv4_specific</span> =</span> &#123;  </span><br><span class="line"> .queue_xmit    = ip_queue_xmit,  </span><br><span class="line"> .send_check    = tcp_v4_send_check,  </span><br><span class="line"> ...  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>


<p>自此，传输层的工作也就都完成了。数据离开了传输层，接下来将会进入到内核在网络层的实现里。</p>
<h3 id="4-3-网络层发送处理"><a href="#4-3-网络层发送处理" class="headerlink" title="4.3 网络层发送处理"></a>4.3 网络层发送处理</h3><p>Linux 内核网络层的发送的实现位于 net&#x2F;ipv4&#x2F;ip_output.c 这个文件。传输层调用到的 ip_queue_xmit 也在这里。（从文件名上也能看出来进入到 IP 层了，源文件名已经从 tcp_xxx 变成了 ip_xxx。）</p>
<p>在网络层里主要处理路由项查找、IP 头设置、netfilter 过滤、skb 切分（大于 MTU 的话）等几项工作，处理完这些工作后会交给更下层的邻居子系统来处理。</p>
<p><img src="/img/wiki-linux-network/14.jpg"></p>
<p>我们来看网络层入口函数 ip_queue_xmit 的源码：  </p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: net/ipv4/ip_output.c  </span></span><br><span class="line"><span class="type">int</span> <span class="title function_">ip_queue_xmit</span><span class="params">(<span class="keyword">struct</span> sk_buff *skb, <span class="keyword">struct</span> flowi *fl)</span>  </span><br><span class="line">&#123;  </span><br><span class="line"> <span class="comment">//检查 socket 中是否有缓存的路由表  </span></span><br><span class="line"> rt = (<span class="keyword">struct</span> rtable *)__sk_dst_check(sk, <span class="number">0</span>);  </span><br><span class="line"> <span class="keyword">if</span> (rt == <span class="literal">NULL</span>) &#123;  </span><br><span class="line">  <span class="comment">//没有缓存则展开查找  </span></span><br><span class="line">  <span class="comment">//则查找路由项， 并缓存到 socket 中  </span></span><br><span class="line">  rt = ip_route_output_ports(...);  </span><br><span class="line">  sk_setup_caps(sk, &amp;rt-&gt;dst);  </span><br><span class="line"> &#125;  </span><br><span class="line">  </span><br><span class="line"> <span class="comment">//为 skb 设置路由表  </span></span><br><span class="line"> skb_dst_set_noref(skb, &amp;rt-&gt;dst);  </span><br><span class="line">  </span><br><span class="line"> <span class="comment">//设置 IP header  </span></span><br><span class="line"> iph = ip_hdr(skb);  </span><br><span class="line"> iph-&gt;protocol = sk-&gt;sk_protocol;  </span><br><span class="line"> iph-&gt;ttl      = ip_select_ttl(inet, &amp;rt-&gt;dst);  </span><br><span class="line"> iph-&gt;frag_off = ...;  </span><br><span class="line">  </span><br><span class="line"> <span class="comment">//发送  </span></span><br><span class="line"> ip_local_out(skb);  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>


<p>ip_queue_xmit 已经到了网络层，在这个函数里我们看到了网络层相关的功能路由项查找，如果找到了则设置到 skb 上（没有路由的话就直接报错返回了）。</p>
<p>在 Linux 上通过 route 命令可以看到你本机的路由配置。</p>
<p><img src="/img/wiki-linux-network/15.jpg"></p>
<p>在路由表中，可以查到某个目的网络应该通过哪个 Iface（网卡），哪个 Gateway（网卡）发送出去。查找出来以后缓存到 socket 上，下次再发送数据就不用查了。  </p>
<p>接着把路由表地址也放到 skb 里去。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: include/linux/skbuff.h  </span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">sk_buff</span> &#123;</span>  </span><br><span class="line"> <span class="comment">//保存了一些路由相关信息  </span></span><br><span class="line"> <span class="type">unsigned</span> <span class="type">long</span>  _skb_refdst;  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>


<p>接下来就是定位到 skb 里的 IP 头的位置上，然后开始按照协议规范设置 IP header。</p>
<p><img src="/img/wiki-linux-network/16.jpg"></p>
<p>再通过 ip_local_out 进入到下一步的处理。  </p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: net/ipv4/ip_output.c    </span></span><br><span class="line"><span class="type">int</span> <span class="title function_">ip_local_out</span><span class="params">(<span class="keyword">struct</span> sk_buff *skb)</span>  </span><br><span class="line">&#123;  </span><br><span class="line"> <span class="comment">//执行 netfilter 过滤  </span></span><br><span class="line"> err = __ip_local_out(skb);  </span><br><span class="line">  </span><br><span class="line"> <span class="comment">//开始发送数据  </span></span><br><span class="line"> <span class="keyword">if</span> (likely(err == <span class="number">1</span>))  </span><br><span class="line">  err = dst_output(skb);  </span><br><span class="line"> ......  </span><br></pre></td></tr></table></figure>


<p>在 ip_local_out &#x3D;&gt; __ip_local_out &#x3D;&gt; nf_hook 会执行 netfilter 过滤。如果你使用 iptables 配置了一些规则，那么这里将检测是否命中规则。<strong>如果你设置了非常复杂的 netfilter 规则，在这个函数这里将会导致你的进程 CPU 开销会极大增加</strong>。</p>
<p>还是不多展开说，继续只聊和发送有关的过程 dst_output。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: include/net/dst.h  </span></span><br><span class="line"><span class="type">static</span> <span class="keyword">inline</span> <span class="type">int</span> <span class="title function_">dst_output</span><span class="params">(<span class="keyword">struct</span> sk_buff *skb)</span>  </span><br><span class="line">&#123;  </span><br><span class="line"> <span class="keyword">return</span> skb_dst(skb)-&gt;output(skb);  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>


<p>此函数找到到这个 skb 的路由表（dst 条目） ，然后调用路由表的 output 方法。这又是一个函数指针，指向的是 ip_output 方法。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: net/ipv4/ip_output.c  </span></span><br><span class="line"><span class="type">int</span> <span class="title function_">ip_output</span><span class="params">(<span class="keyword">struct</span> sk_buff *skb)</span>  </span><br><span class="line">&#123;  </span><br><span class="line"> <span class="comment">//统计  </span></span><br><span class="line"> .....  </span><br><span class="line">  </span><br><span class="line"> <span class="comment">//再次交给 netfilter，完毕后回调 ip_finish_output  </span></span><br><span class="line"> <span class="keyword">return</span> NF_HOOK_COND(NFPROTO_IPV4, NF_INET_POST_ROUTING, skb, <span class="literal">NULL</span>, dev,  </span><br><span class="line">    ip_finish_output,  </span><br><span class="line">    !(IPCB(skb)-&gt;flags &amp; IPSKB_REROUTED));  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>


<p>在 ip_output 中进行一些简单的，统计工作，再次执行 netfilter 过滤。过滤通过之后回调 ip_finish_output。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: net/ipv4/ip_output.c  </span></span><br><span class="line"><span class="type">static</span> <span class="type">int</span> <span class="title function_">ip_finish_output</span><span class="params">(<span class="keyword">struct</span> sk_buff *skb)</span>  </span><br><span class="line">&#123;  </span><br><span class="line"> <span class="comment">//大于 mtu 的话就要进行分片了  </span></span><br><span class="line"> <span class="keyword">if</span> (skb-&gt;len &gt; ip_skb_dst_mtu(skb) &amp;&amp; !skb_is_gso(skb))  </span><br><span class="line">  <span class="keyword">return</span> ip_fragment(skb, ip_finish_output2);  </span><br><span class="line"> <span class="keyword">else</span>  </span><br><span class="line">  <span class="keyword">return</span> ip_finish_output2(skb);  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>


<p>在 ip_finish_output 中我们看到，<strong>如果数据大于 MTU 的话，是会执行分片的。</strong></p>
<blockquote>
<p>实际 MTU 大小确定依赖 MTU 发现，以太网帧为 1500 字节。之前 QQ 团队在早期的时候，会尽量控制自己数据包尺寸小于 MTU，通过这种方式来优化网络性能。因为分片会带来两个问题：1、需要进行额外的切分处理，有额外性能开销。2、只要一个分片丢失，整个包都得重传。所以避免分片既杜绝了分片开销，也大大降低了重传率。</p>
</blockquote>
<p>在 ip_finish_output2 中，终于发送过程会进入到下一层，邻居子系统中。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: net/ipv4/ip_output.c  </span></span><br><span class="line"><span class="type">static</span> <span class="keyword">inline</span> <span class="type">int</span> <span class="title function_">ip_finish_output2</span><span class="params">(<span class="keyword">struct</span> sk_buff *skb)</span>  </span><br><span class="line">&#123;  </span><br><span class="line"> <span class="comment">//根据下一跳 IP 地址查找邻居项，找不到就创建一个  </span></span><br><span class="line"> nexthop = (__force u32) rt_nexthop(rt, ip_hdr(skb)-&gt;daddr);    </span><br><span class="line"> neigh = __ipv4_neigh_lookup_noref(dev, nexthop);  </span><br><span class="line"> <span class="keyword">if</span> (unlikely(!neigh))  </span><br><span class="line">  neigh = __neigh_create(&amp;arp_tbl, &amp;nexthop, dev, <span class="literal">false</span>);  </span><br><span class="line">  </span><br><span class="line"> <span class="comment">//继续向下层传递  </span></span><br><span class="line"> <span class="type">int</span> res = dst_neigh_output(dst, neigh, skb);  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>


<h3 id="4-4-邻居子系统"><a href="#4-4-邻居子系统" class="headerlink" title="4.4 邻居子系统"></a>4.4 邻居子系统</h3><p>邻居子系统是位于网络层和数据链路层中间的一个系统，其作用是对网络层提供一个封装，让网络层不必关心下层的地址信息，让下层来决定发送到哪个 MAC 地址。</p>
<p>而且这个邻居子系统并不位于协议栈 net&#x2F;ipv4&#x2F; 目录内，而是位于 net&#x2F;core&#x2F;neighbour.c。因为无论是对于 IPv4 还是 IPv6 ，都需要使用该模块。</p>
<p><img src="/img/wiki-linux-network/17.jpg"></p>
<p>在邻居子系统里主要是查找或者创建邻居项，在创造邻居项的时候，有可能会发出实际的 arp 请求。然后封装一下 MAC 头，将发送过程再传递到更下层的网络设备子系统。大致流程如图。  </p>
<p><img src="/img/wiki-linux-network/18.jpg"></p>
<p>理解了大致流程，我们再回头看源码。在上面小节 ip_finish_output2 源码中调用了 __ipv4_neigh_lookup_noref。它是在 arp 缓存中进行查找，其第二个参数传入的是路由下一跳 IP 信息。  </p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: include/net/arp.h  </span></span><br><span class="line"><span class="keyword">extern</span> <span class="class"><span class="keyword">struct</span> <span class="title">neigh_table</span> <span class="title">arp_tbl</span>;</span>  </span><br><span class="line"><span class="type">static</span> <span class="keyword">inline</span> <span class="class"><span class="keyword">struct</span> <span class="title">neighbour</span> *__<span class="title">ipv4_neigh_lookup_noref</span>(  </span></span><br><span class="line"><span class="class"> <span class="keyword">struct</span> <span class="title">net_device</span> *<span class="title">dev</span>, <span class="title">u32</span> <span class="title">key</span>)  </span></span><br><span class="line"><span class="class">&#123;</span>  </span><br><span class="line"> <span class="class"><span class="keyword">struct</span> <span class="title">neigh_hash_table</span> *<span class="title">nht</span> =</span> rcu_dereference_bh(arp_tbl.nht);  </span><br><span class="line">  </span><br><span class="line"> <span class="comment">//计算 hash 值，加速查找  </span></span><br><span class="line"> hash_val = arp_hashfn(......);  </span><br><span class="line"> <span class="keyword">for</span> (n = rcu_dereference_bh(nht-&gt;hash_buckets[hash_val]);  </span><br><span class="line">   n != <span class="literal">NULL</span>;  </span><br><span class="line">   n = rcu_dereference_bh(n-&gt;next)) &#123;  </span><br><span class="line">  <span class="keyword">if</span> (n-&gt;dev == dev &amp;&amp; *(u32 *)n-&gt;primary_key == key)  </span><br><span class="line">   <span class="keyword">return</span> n;  </span><br><span class="line"> &#125;  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>


<p>如果查找不到，则调用 __neigh_create 创建一个邻居。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: net/core/neighbour.c  </span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">neighbour</span> *__<span class="title">neigh_create</span>(......)  </span></span><br><span class="line"><span class="class">&#123;</span>  </span><br><span class="line"> <span class="comment">//申请邻居表项  </span></span><br><span class="line"> <span class="class"><span class="keyword">struct</span> <span class="title">neighbour</span> *<span class="title">n1</span>, *<span class="title">rc</span>, *<span class="title">n</span> =</span> neigh_alloc(tbl, dev);  </span><br><span class="line">  </span><br><span class="line"> <span class="comment">//构造赋值  </span></span><br><span class="line"> <span class="built_in">memcpy</span>(n-&gt;primary_key, pkey, key_len);  </span><br><span class="line"> n-&gt;dev = dev;  </span><br><span class="line"> n-&gt;parms-&gt;neigh_setup(n);  </span><br><span class="line">  </span><br><span class="line"> <span class="comment">//最后添加到邻居 hashtable 中  </span></span><br><span class="line"> rcu_assign_pointer(nht-&gt;hash_buckets[hash_val], n);  </span><br><span class="line"> ......  </span><br></pre></td></tr></table></figure>


<p>有了邻居项以后，此时仍然还不具备发送 IP 报文的能力，因为目的 MAC 地址还未获取。调用 dst_neigh_output 继续传递 skb。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: include/net/dst.h  </span></span><br><span class="line"><span class="type">static</span> <span class="keyword">inline</span> <span class="type">int</span> <span class="title function_">dst_neigh_output</span><span class="params">(<span class="keyword">struct</span> dst_entry *dst,   </span></span><br><span class="line"><span class="params">     <span class="keyword">struct</span> neighbour *n, <span class="keyword">struct</span> sk_buff *skb)</span>  </span><br><span class="line">&#123;  </span><br><span class="line"> ......  </span><br><span class="line"> <span class="keyword">return</span> n-&gt;output(n, skb);  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>


<p>调用 output，实际指向的是 neigh_resolve_output。在这个函数内部有可能会发出 arp 网络请求。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: net/core/neighbour.c  </span></span><br><span class="line"><span class="type">int</span> <span class="title function_">neigh_resolve_output</span><span class="params">()</span>&#123;  </span><br><span class="line">  </span><br><span class="line"> <span class="comment">//注意：这里可能会触发 arp 请求  </span></span><br><span class="line"> <span class="keyword">if</span> (!neigh_event_send(neigh, skb)) &#123;  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">//neigh-&gt;ha 是 MAC 地址  </span></span><br><span class="line">  dev_hard_header(skb, dev, ntohs(skb-&gt;protocol),  </span><br><span class="line">           neigh-&gt;ha, <span class="literal">NULL</span>, skb-&gt;len);  </span><br><span class="line">  <span class="comment">//发送  </span></span><br><span class="line">  dev_queue_xmit(skb);  </span><br><span class="line"> &#125;  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>


<p>当获取到硬件 MAC 地址以后，就可以封装 skb 的 MAC 头了。最后调用 dev_queue_xmit 将 skb 传递给 Linux 网络设备子系统。</p>
<h3 id="4-5-网络设备子系统"><a href="#4-5-网络设备子系统" class="headerlink" title="4.5 网络设备子系统"></a>4.5 网络设备子系统</h3><p><img src="/img/wiki-linux-network/19.jpg"></p>
<p>邻居子系统通过 dev_queue_xmit 进入到网络设备子系统中来。  </p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: net/core/dev.c   </span></span><br><span class="line"><span class="type">int</span> <span class="title function_">dev_queue_xmit</span><span class="params">(<span class="keyword">struct</span> sk_buff *skb)</span>  </span><br><span class="line">&#123;  </span><br><span class="line"> <span class="comment">//选择发送队列  </span></span><br><span class="line"> txq = netdev_pick_tx(dev, skb);  </span><br><span class="line">  </span><br><span class="line"> <span class="comment">//获取与此队列关联的排队规则  </span></span><br><span class="line"> q = rcu_dereference_bh(txq-&gt;qdisc);  </span><br><span class="line">  </span><br><span class="line"> <span class="comment">//如果有队列，则调用__dev_xmit_skb 继续处理数据  </span></span><br><span class="line"> <span class="keyword">if</span> (q-&gt;enqueue) &#123;  </span><br><span class="line">  rc = __dev_xmit_skb(skb, q, dev, txq);  </span><br><span class="line">  <span class="keyword">goto</span> out;  </span><br><span class="line"> &#125;  </span><br><span class="line">  </span><br><span class="line"> <span class="comment">//没有队列的是回环设备和隧道设备  </span></span><br><span class="line"> ......  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>


<p>开篇第二节网卡启动准备里我们说过，网卡是有多个发送队列的（尤其是现在的网卡）。上面对 netdev_pick_tx 函数的调用就是选择一个队列进行发送。</p>
<p>netdev_pick_tx 发送队列的选择受 XPS 等配置的影响，而且还有缓存，也是一套小复杂的逻辑。这里我们只关注两个逻辑，首先会获取用户的 XPS 配置，否则就自动计算了。代码见 netdev_pick_tx &#x3D;&gt; __netdev_pick_tx。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: net/core/flow_dissector.c  </span></span><br><span class="line">u16 __netdev_pick_tx(<span class="keyword">struct</span> net_device *dev, <span class="keyword">struct</span> sk_buff *skb)  </span><br><span class="line">&#123;  </span><br><span class="line"> <span class="comment">//获取 XPS 配置  </span></span><br><span class="line"> <span class="type">int</span> new_index = get_xps_queue(dev, skb);  </span><br><span class="line">  </span><br><span class="line"> <span class="comment">//自动计算队列  </span></span><br><span class="line"> <span class="keyword">if</span> (new_index &lt; <span class="number">0</span>)  </span><br><span class="line">  new_index = skb_tx_hash(dev, skb);&#125;  </span><br></pre></td></tr></table></figure>


<p>然后获取与此队列关联的 qdisc。在 linux 上通过 tc 命令可以看到 qdisc 类型，例如对于我的某台多队列网卡机器上是 mq disc。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#tc qdisc  </span><br><span class="line">qdisc mq 0: dev eth0 root  </span><br></pre></td></tr></table></figure>


<p>大部分的设备都有队列（回环设备和隧道设备除外），所以现在我们进入到 __dev_xmit_skb。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: net/core/dev.c  </span></span><br><span class="line"><span class="type">static</span> <span class="keyword">inline</span> <span class="type">int</span> __dev_xmit_skb(<span class="keyword">struct</span> sk_buff *skb, <span class="keyword">struct</span> Qdisc *q,  </span><br><span class="line">     <span class="keyword">struct</span> net_device *dev,  </span><br><span class="line">     <span class="keyword">struct</span> netdev_queue *txq)  </span><br><span class="line">&#123;  </span><br><span class="line"> <span class="comment">//1.如果可以绕开排队系统  </span></span><br><span class="line"> <span class="keyword">if</span> ((q-&gt;flags &amp; TCQ_F_CAN_BYPASS) &amp;&amp; !qdisc_qlen(q) &amp;&amp;  </span><br><span class="line">     qdisc_run_begin(q)) &#123;  </span><br><span class="line">  ......  </span><br><span class="line"> &#125;  </span><br><span class="line">  </span><br><span class="line"> <span class="comment">//2.正常排队  </span></span><br><span class="line"> <span class="keyword">else</span> &#123;  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">//入队  </span></span><br><span class="line">  q-&gt;enqueue(skb, q)  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">//开始发送  </span></span><br><span class="line">  __qdisc_run(q);  </span><br><span class="line"> &#125;  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>


<p>上述代码中分两种情况，1 是可以 bypass（绕过）排队系统的，另外一种是正常排队。我们只看第二种情况。</p>
<p>先调用 q-&gt;enqueue 把 skb 添加到队列里。然后调用 __qdisc_run 开始发送。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: net/sched/sch_generic.c  </span></span><br><span class="line"><span class="type">void</span> __qdisc_run(<span class="keyword">struct</span> Qdisc *q)  </span><br><span class="line">&#123;  </span><br><span class="line"> <span class="type">int</span> quota = weight_p;  </span><br><span class="line">  </span><br><span class="line"> <span class="comment">//循环从队列取出一个 skb 并发送  </span></span><br><span class="line"> <span class="keyword">while</span> (qdisc_restart(q)) &#123;  </span><br><span class="line">    </span><br><span class="line">  <span class="comment">// 如果发生下面情况之一，则延后处理：  </span></span><br><span class="line">  <span class="comment">// 1. quota 用尽  </span></span><br><span class="line">  <span class="comment">// 2. 其他进程需要 CPU  </span></span><br><span class="line">  <span class="keyword">if</span> (--quota &lt;= <span class="number">0</span> || need_resched()) &#123;  </span><br><span class="line">   <span class="comment">//将触发一次 NET_TX_SOFTIRQ 类型 softirq  </span></span><br><span class="line">   __netif_schedule(q);  </span><br><span class="line">   <span class="keyword">break</span>;  </span><br><span class="line">  &#125;  </span><br><span class="line"> &#125;  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>


<p>在上述代码中，我们看到 while 循环不断地从队列中取出 skb 并进行发送。注意，这个时候其实都占用的是用户进程的系统态时间(sy)。只有当 quota 用尽或者其它进程需要 CPU 的时候才触发软中断进行发送。</p>
<p><strong>所以这就是为什么一般服务器上查看 &#x2F;proc&#x2F;softirqs，一般 NET_RX 都要比 NET_TX 大的多的第二个原因</strong>。对于读来说，都是要经过 NET_RX 软中断，而对于发送来说，只有系统态配额用尽才让软中断上。</p>
<p>我们来把精力在放到 qdisc_restart 上，继续看发送过程。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">static inline int qdisc_restart(struct Qdisc *q)  </span><br><span class="line">&#123;  </span><br><span class="line"> //从 qdisc 中取出要发送的 skb  </span><br><span class="line"> skb = dequeue_skb(q);  </span><br><span class="line"> ...  </span><br><span class="line">  </span><br><span class="line"> return sch_direct_xmit(skb, q, dev, txq, root_lock);  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>


<p>qdisc_restart 从队列中取出一个 skb，并调用 sch_direct_xmit 继续发送。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: net/sched/sch_generic.c  </span></span><br><span class="line"><span class="type">int</span> <span class="title function_">sch_direct_xmit</span><span class="params">(<span class="keyword">struct</span> sk_buff *skb, <span class="keyword">struct</span> Qdisc *q,  </span></span><br><span class="line"><span class="params">   <span class="keyword">struct</span> net_device *dev, <span class="keyword">struct</span> netdev_queue *txq,  </span></span><br><span class="line"><span class="params">   <span class="type">spinlock_t</span> *root_lock)</span>  </span><br><span class="line">&#123;  </span><br><span class="line"> <span class="comment">//调用驱动程序来发送数据  </span></span><br><span class="line"> ret = dev_hard_start_xmit(skb, dev, txq);  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>


<h3 id="4-6-软中断调度"><a href="#4-6-软中断调度" class="headerlink" title="4.6 软中断调度"></a>4.6 软中断调度</h3><p>在 4.5 咱们看到了如果系统态 CPU 发送网络包不够用的时候，会调用 __netif_schedule 触发一个软中断。该函数会进入到 __netif_reschedule，由它来实际发出 NET_TX_SOFTIRQ 类型软中断。</p>
<p>软中断是由内核线程来运行的，该线程会进入到 net_tx_action 函数，在该函数中能获取到发送队列，并也最终调用到驱动程序里的入口函数 dev_hard_start_xmit。</p>
<p><img src="/img/wiki-linux-network/20.jpg"></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: net/core/dev.c  </span></span><br><span class="line"><span class="type">static</span> <span class="keyword">inline</span> <span class="type">void</span> __netif_reschedule(<span class="keyword">struct</span> Qdisc *q)  </span><br><span class="line">&#123;  </span><br><span class="line"> sd = &amp;__get_cpu_var(softnet_data);  </span><br><span class="line"> q-&gt;next_sched = <span class="literal">NULL</span>;  </span><br><span class="line"> *sd-&gt;output_queue_tailp = q;  </span><br><span class="line"> sd-&gt;output_queue_tailp = &amp;q-&gt;next_sched;  </span><br><span class="line">  </span><br><span class="line"> ......  </span><br><span class="line"> raise_softirq_irqoff(NET_TX_SOFTIRQ);  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>


<p>在该函数里在软中断能访问到的 softnet_data 里设置了要发送的数据队列，添加到了 output_queue 里了。紧接着触发了 NET_TX_SOFTIRQ 类型的软中断。（T 代表 transmit 传输）</p>
<p>软中断的入口代码我这里也不详细扒了，感兴趣的同学参考<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MjM5Njg5NDgwNA==&mid=2247484058&idx=1&sn=a2621bc27c74b313528eefbc81ee8c0f&scene=21#wechat_redirect">《图解Linux网络包接收过程》</a>一文中的 3.2 小节 - ksoftirqd内核线程处理软中断。</p>
<p>我们直接从 NET_TX_SOFTIRQ softirq 注册的回调函数 net_tx_action讲起。用户态进程触发完软中断之后，会有一个软中断内核线程会执行到 net_tx_action。</p>
<p><strong>牢记，这以后发送数据消耗的 CPU 就都显示在 si 这里了，不会消耗用户进程的系统时间了</strong>。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: net/core/dev.c  </span></span><br><span class="line"><span class="type">static</span> <span class="type">void</span> <span class="title function_">net_tx_action</span><span class="params">(<span class="keyword">struct</span> softirq_action *h)</span>  </span><br><span class="line">&#123;  </span><br><span class="line"> <span class="comment">//通过 softnet_data 获取发送队列  </span></span><br><span class="line"> <span class="class"><span class="keyword">struct</span> <span class="title">softnet_data</span> *<span class="title">sd</span> =</span> &amp;__get_cpu_var(softnet_data);  </span><br><span class="line">  </span><br><span class="line"> <span class="comment">// 如果 output queue 上有 qdisc  </span></span><br><span class="line"> <span class="keyword">if</span> (sd-&gt;output_queue) &#123;  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 将 head 指向第一个 qdisc  </span></span><br><span class="line">  head = sd-&gt;output_queue;  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">//遍历 qdsics 列表  </span></span><br><span class="line">  <span class="keyword">while</span> (head) &#123;  </span><br><span class="line">   <span class="class"><span class="keyword">struct</span> <span class="title">Qdisc</span> *<span class="title">q</span> =</span> head;  </span><br><span class="line">   head = head-&gt;next_sched;  </span><br><span class="line">  </span><br><span class="line">   <span class="comment">//发送数据  </span></span><br><span class="line">   qdisc_run(q);  </span><br><span class="line">  &#125;  </span><br><span class="line"> &#125;  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>


<p>软中断这里会获取 softnet_data。前面我们看到进程内核态在调用 __netif_reschedule 的时候把发送队列写到 softnet_data 的 output_queue 里了。软中断循环遍历 sd-&gt;output_queue 发送数据帧。</p>
<p>来看 qdisc_run，它和进程用户态一样，也会调用到 __qdisc_run。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: include/net/pkt_sched.h  </span></span><br><span class="line"><span class="type">static</span> <span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">qdisc_run</span><span class="params">(<span class="keyword">struct</span> Qdisc *q)</span>  </span><br><span class="line">&#123;  </span><br><span class="line"> <span class="keyword">if</span> (qdisc_run_begin(q))  </span><br><span class="line">  __qdisc_run(q);  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>


<p>然后一样就是进入 qdisc_restart &#x3D;&gt; sch_direct_xmit，直到驱动程序函数 dev_hard_start_xmit。</p>
<h3 id="4-7-igb-网卡驱动发送"><a href="#4-7-igb-网卡驱动发送" class="headerlink" title="4.7 igb 网卡驱动发送"></a>4.7 igb 网卡驱动发送</h3><p>我们前面看到，无论是对于用户进程的内核态，还是对于软中断上下文，都会调用到网络设备子系统中的 dev_hard_start_xmit 函数。在这个函数中，会调用到驱动里的发送函数 igb_xmit_frame。</p>
<p>在驱动函数里，将 skb 会挂到 RingBuffer上，驱动调用完毕后，数据包将真正从网卡发送出去。</p>
<p><img src="/img/wiki-linux-network/21.jpg"></p>
<p>我们来看看实际的源码：  </p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: net/core/dev.c  </span></span><br><span class="line"><span class="type">int</span> <span class="title function_">dev_hard_start_xmit</span><span class="params">(<span class="keyword">struct</span> sk_buff *skb, <span class="keyword">struct</span> net_device *dev,  </span></span><br><span class="line"><span class="params">   <span class="keyword">struct</span> netdev_queue *txq)</span>  </span><br><span class="line">&#123;  </span><br><span class="line"> <span class="comment">//获取设备的回调函数集合 ops  </span></span><br><span class="line"> <span class="type">const</span> <span class="class"><span class="keyword">struct</span> <span class="title">net_device_ops</span> *<span class="title">ops</span> =</span> dev-&gt;netdev_ops;  </span><br><span class="line">  </span><br><span class="line"> <span class="comment">//获取设备支持的功能列表  </span></span><br><span class="line"> features = netif_skb_features(skb);  </span><br><span class="line">  </span><br><span class="line"> <span class="comment">//调用驱动的 ops 里面的发送回调函数 ndo_start_xmit 将数据包传给网卡设备  </span></span><br><span class="line"> skb_len = skb-&gt;len;  </span><br><span class="line"> rc = ops-&gt;ndo_start_xmit(skb, dev);  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>


<p>其中 ndo_start_xmit 是网卡驱动要实现的一个函数，是在 net_device_ops 中定义的。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: include/linux/netdevice.h  </span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">net_device_ops</span> &#123;</span>  </span><br><span class="line"> <span class="type">netdev_tx_t</span>  (*ndo_start_xmit) (<span class="keyword">struct</span> sk_buff *skb,  </span><br><span class="line">         <span class="keyword">struct</span> net_device *dev);  </span><br><span class="line">  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>


<p>在 igb 网卡驱动源码中，我们找到了。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: drivers/net/ethernet/intel/igb/igb_main.c  </span></span><br><span class="line"><span class="type">static</span> <span class="type">const</span> <span class="class"><span class="keyword">struct</span> <span class="title">net_device_ops</span> <span class="title">igb_netdev_ops</span> =</span> &#123;  </span><br><span class="line"> .ndo_open  = igb_open,  </span><br><span class="line"> .ndo_stop  = igb_close,  </span><br><span class="line"> .ndo_start_xmit  = igb_xmit_frame,   </span><br><span class="line"> ...  </span><br><span class="line">&#125;;  </span><br></pre></td></tr></table></figure>


<p>也就是说，对于网络设备层定义的 ndo_start_xmit， igb 的实现函数是 igb_xmit_frame。这个函数是在网卡驱动初始化的时候被赋值的。具体初始化过程参见<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MjM5Njg5NDgwNA==&mid=2247484058&idx=1&sn=a2621bc27c74b313528eefbc81ee8c0f&scene=21#wechat_redirect">《图解Linux网络包接收过程》</a>一文中的 2.4 节，网卡驱动初始化。</p>
<p>所以在上面网络设备层调用 ops-&gt;ndo_start_xmit 的时候，会实际上进入 igb_xmit_frame 这个函数中。我们进入这个函数来看看驱动程序是如何工作的。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: drivers/net/ethernet/intel/igb/igb_main.c  </span></span><br><span class="line"><span class="type">static</span> <span class="type">netdev_tx_t</span> <span class="title function_">igb_xmit_frame</span><span class="params">(<span class="keyword">struct</span> sk_buff *skb,  </span></span><br><span class="line"><span class="params">      <span class="keyword">struct</span> net_device *netdev)</span>  </span><br><span class="line">&#123;  </span><br><span class="line"> ......  </span><br><span class="line"> <span class="keyword">return</span> igb_xmit_frame_ring(skb, igb_tx_queue_mapping(adapter, skb));  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="type">netdev_tx_t</span> <span class="title function_">igb_xmit_frame_ring</span><span class="params">(<span class="keyword">struct</span> sk_buff *skb,  </span></span><br><span class="line"><span class="params">    <span class="keyword">struct</span> igb_ring *tx_ring)</span>  </span><br><span class="line">&#123;  </span><br><span class="line"> <span class="comment">//获取TX Queue 中下一个可用缓冲区信息  </span></span><br><span class="line"> first = &amp;tx_ring-&gt;tx_buffer_info[tx_ring-&gt;next_to_use];  </span><br><span class="line"> first-&gt;skb = skb;  </span><br><span class="line"> first-&gt;bytecount = skb-&gt;len;  </span><br><span class="line"> first-&gt;gso_segs = <span class="number">1</span>;  </span><br><span class="line">  </span><br><span class="line"> <span class="comment">//igb_tx_map 函数准备给设备发送的数据。  </span></span><br><span class="line"> igb_tx_map(tx_ring, first, hdr_len);  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>


<p>在这里从网卡的发送队列的 RingBuffer 中取下来一个元素，并将 skb 挂到元素上。</p>
<p><img src="/img/wiki-linux-network/22.jpg"></p>
<p>igb_tx_map 函数处理将 skb 数据映射到网卡可访问的内存 DMA 区域。  </p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: drivers/net/ethernet/intel/igb/igb_main.c  </span></span><br><span class="line"><span class="type">static</span> <span class="type">void</span> <span class="title function_">igb_tx_map</span><span class="params">(<span class="keyword">struct</span> igb_ring *tx_ring,  </span></span><br><span class="line"><span class="params">      <span class="keyword">struct</span> igb_tx_buffer *first,  </span></span><br><span class="line"><span class="params">      <span class="type">const</span> u8 hdr_len)</span>  </span><br><span class="line">&#123;  </span><br><span class="line"> <span class="comment">//获取下一个可用描述符指针  </span></span><br><span class="line"> tx_desc = IGB_TX_DESC(tx_ring, i);  </span><br><span class="line">  </span><br><span class="line"> <span class="comment">//为 skb-&gt;data 构造内存映射，以允许设备通过 DMA 从 RAM 中读取数据  </span></span><br><span class="line"> dma = dma_map_single(tx_ring-&gt;dev, skb-&gt;data, size, DMA_TO_DEVICE);  </span><br><span class="line">  </span><br><span class="line"> <span class="comment">//遍历该数据包的所有分片,为 skb 的每个分片生成有效映射  </span></span><br><span class="line"> <span class="keyword">for</span> (frag = &amp;skb_shinfo(skb)-&gt;frags[<span class="number">0</span>];; frag++) &#123;  </span><br><span class="line">  </span><br><span class="line">  tx_desc-&gt;read.buffer_addr = cpu_to_le64(dma);  </span><br><span class="line">  tx_desc-&gt;read.cmd_type_len = ...;  </span><br><span class="line">  tx_desc-&gt;read.olinfo_status = <span class="number">0</span>;  </span><br><span class="line"> &#125;  </span><br><span class="line">  </span><br><span class="line"> <span class="comment">//设置最后一个descriptor  </span></span><br><span class="line"> cmd_type |= size | IGB_TXD_DCMD;  </span><br><span class="line"> tx_desc-&gt;read.cmd_type_len = cpu_to_le32(cmd_type);  </span><br><span class="line">  </span><br><span class="line"> <span class="comment">/* Force memory writes to complete before letting h/w know there  </span></span><br><span class="line"><span class="comment">  * are new descriptors to fetch  </span></span><br><span class="line"><span class="comment">  */</span>  </span><br><span class="line"> wmb();  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>


<p>当所有需要的描述符都已建好，且 skb 的所有数据都映射到 DMA 地址后，驱动就会进入到它的最后一步，触发真实的发送。</p>
<h3 id="4-8-发送完成硬中断"><a href="#4-8-发送完成硬中断" class="headerlink" title="4.8 发送完成硬中断"></a>4.8 发送完成硬中断</h3><p>当数据发送完成以后，其实工作并没有结束。因为内存还没有清理。当发送完成的时候，网卡设备会触发一个硬中断来释放内存。</p>
<p>在<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MjM5Njg5NDgwNA==&mid=2247484058&idx=1&sn=a2621bc27c74b313528eefbc81ee8c0f&scene=21#wechat_redirect">《图解Linux网络包接收过程》</a> 一文中的 3.1 和 3.2 小节，我们详细讲述过硬中断和软中断的处理过程。</p>
<p>在发送完成硬中断里，会执行 RingBuffer 内存的清理工作，如图。</p>
<p><img src="/img/wiki-linux-network/23.jpg"></p>
<p>再回头看一下硬中断触发软中断的源码。  </p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: drivers/net/ethernet/intel/igb/igb_main.c  </span></span><br><span class="line"><span class="type">static</span> <span class="keyword">inline</span> <span class="type">void</span> ____napi_schedule(...)&#123;  </span><br><span class="line"> list_add_tail(&amp;napi-&gt;poll_list, &amp;sd-&gt;poll_list);  </span><br><span class="line"> __raise_softirq_irqoff(NET_RX_SOFTIRQ);  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>


<p>这里有个很有意思的细节，无论硬中断是因为是有数据要接收，还是说发送完成通知，<strong>从硬中断触发的软中断都是 NET_RX_SOFTIRQ</strong>。这个我们在第一节说过了，这是软中断统计中 RX 要高于 TX 的一个原因。</p>
<p>好我们接着进入软中断的回调函数 igb_poll。在这个函数里，我们注意到有一行 igb_clean_tx_irq，参见源码：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: drivers/net/ethernet/intel/igb/igb_main.c  </span></span><br><span class="line"><span class="type">static</span> <span class="type">int</span> <span class="title function_">igb_poll</span><span class="params">(<span class="keyword">struct</span> napi_struct *napi, <span class="type">int</span> budget)</span>  </span><br><span class="line">&#123;  </span><br><span class="line"> <span class="comment">//performs the transmit completion operations  </span></span><br><span class="line"> <span class="keyword">if</span> (q_vector-&gt;tx.ring)  </span><br><span class="line">  clean_complete = igb_clean_tx_irq(q_vector);  </span><br><span class="line"> ...  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>


<p>我们来看看当传输完成的时候，igb_clean_tx_irq 都干啥了。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: drivers/net/ethernet/intel/igb/igb_main.c  </span></span><br><span class="line"><span class="type">static</span> <span class="type">bool</span> <span class="title function_">igb_clean_tx_irq</span><span class="params">(<span class="keyword">struct</span> igb_q_vector *q_vector)</span>  </span><br><span class="line">&#123;  </span><br><span class="line"> <span class="comment">//free the skb  </span></span><br><span class="line"> dev_kfree_skb_any(tx_buffer-&gt;skb);  </span><br><span class="line">  </span><br><span class="line"> <span class="comment">//clear tx_buffer data  </span></span><br><span class="line"> tx_buffer-&gt;skb = <span class="literal">NULL</span>;  </span><br><span class="line"> dma_unmap_len_set(tx_buffer, len, <span class="number">0</span>);  </span><br><span class="line">  </span><br><span class="line"> <span class="comment">// clear last DMA location and unmap remaining buffers */  </span></span><br><span class="line"> <span class="keyword">while</span> (tx_desc != eop_desc) &#123;  </span><br><span class="line"> &#125;  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>


<p>无非就是清理了 skb，解除了 DMA 映射等等。到了这一步，传输才算是基本完成了。</p>
<p>为啥我说是基本完成，而不是全部完成了呢？因为传输层需要保证可靠性，所以 skb 其实还没有删除。它得等收到对方的 ACK 之后才会真正删除，那个时候才算是彻底的发送完毕。</p>
<h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>用一张图总结一下整个发送过程</p>
<p><img src="/img/wiki-linux-network/24.jpg"></p>
<p>了解了整个发送过程以后，我们回头再来回顾开篇提到的几个问题。  </p>
<p><strong>1.我们在监控内核发送数据消耗的 CPU 时，是应该看 sy 还是 si ？</strong></p>
<p>在网络包的发送过程中，用户进程（在内核态）完成了绝大部分的工作，甚至连调用驱动的事情都干了。只有当内核态进程被切走前才会发起软中断。发送过程中，绝大部分（90%）以上的开销都是在用户进程内核态消耗掉的。</p>
<p>只有一少部分情况下才会触发软中断（NET_TX 类型），由软中断 ksoftirqd 内核进程来发送。</p>
<p>所以，在监控网络 IO 对服务器造成的 CPU 开销的时候，不能仅仅只看 si，而是应该把 si、sy 都考虑进来。</p>
<p><strong>2. 在服务器上查看 &#x2F;proc&#x2F;softirqs，为什么 NET_RX 要比 NET_TX 大的多的多？</strong></p>
<p>之前我认为 NET_RX 是读取，NET_TX 是传输。对于一个既收取用户请求，又给用户返回的 Server 来说。这两块的数字应该差不多才对，至少不会有数量级的差异。但事实上，飞哥手头的一台服务器是这样的：</p>
<p><img src="/img/wiki-linux-network/25.jpg"></p>
<p>经过今天的源码分析，发现这个问题的原因有两个。  </p>
<p>第一个原因是当数据发送完成以后，通过硬中断的方式来通知驱动发送完毕。但是硬中断无论是有数据接收，还是对于发送完毕，触发的软中断都是 NET_RX_SOFTIRQ，而并不是 NET_TX_SOFTIRQ。</p>
<p>第二个原因是对于读来说，都是要经过 NET_RX 软中断的，都走 ksoftirqd 内核进程。而对于发送来说，绝大部分工作都是在用户进程内核态处理了，只有系统态配额用尽才会发出 NET_TX，让软中断上。</p>
<p>综上两个原因，那么在机器上查看 NET_RX 比 NET_TX 大的多就不难理解了。</p>
<p><strong>3.发送网络数据的时候都涉及到哪些内存拷贝操作？</strong></p>
<p>这里的内存拷贝，我们只特指待发送数据的内存拷贝。</p>
<p>第一次拷贝操作是内核申请完 skb 之后，这时候会将用户传递进来的 buffer 里的数据内容都拷贝到 skb 中。如果要发送的数据量比较大的话，这个拷贝操作开销还是不小的。</p>
<p>第二次拷贝操作是从传输层进入网络层的时候，每一个 skb 都会被克隆一个新的副本出来。网络层以及下面的驱动、软中断等组件在发送完成的时候会将这个副本删除。传输层保存着原始的 skb，在当网络对方没有 ack 的时候，还可以重新发送，以实现 TCP 中要求的可靠传输。</p>
<p>第三次拷贝不是必须的，只有当 IP 层发现 skb 大于 MTU 时才需要进行。会再申请额外的 skb，并将原来的 skb 拷贝为多个小的 skb。</p>
<blockquote>
<p>这里插入个题外话，大家在网络性能优化中经常听到的零拷贝，我觉得这有点点夸张的成分。TCP 为了保证可靠性，第二次的拷贝根本就没法省。如果包再大于 MTU 的话，分片时的拷贝同样也避免不了。</p>
</blockquote>
<p>看到这里，相信内核发送数据包对于你来说，已经不再是一个完全不懂的黑盒了。本文哪怕你只看懂十分之一，你也已经掌握了这个黑盒的打开方式。这在你将来优化网络性能时你就会知道从哪儿下手了。</p>
<article class="message message-immersive is-primary">
<div class="message-body">
<i class="fas fa-globe-asia mr-2"></i>本文来自
<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?src=11&timestamp=1648348007&ver=3701&signature=p7x42p0TMbmvcrEhsruNSKixh*Hfmo1YMRAMH-3HJy7*4BmABSznIlyhkpUbqN5n726JX-IWh6m6q2vdiUZvcTCoUhfI9QYnCSuRgwEFK0ctKR6qW2hA*3MA9ofOG5Na&new=1">开发内功修炼</a>.
</div>
</article></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Linux%E7%BD%91%E7%BB%9C/">Linux网络</a></div><!--!--><div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6757924689439593" crossOrigin="anonymous"></script><ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-6757924689439593" data-ad-slot="2352785969"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/pages/wiki-linux-network-recv/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">一文了解 Linux 网络包接收过程</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/pages/bytedance-serverless-frontend/"><span class="level-item">字节跳动基于 Serverless 的前端研发模式升级</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--><div style="width:100%"><script src="https://giscus.app/client.js" data-repo="thetechstack/thetechstack.github.io" data-repo-id="R_kgDOG6Ti1Q" data-category="Announcements" data-category-id="DIC_kwDOG6Ti1c4COQtm" data-mapping="pathname" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="zh-CN" crossOrigin="anonymous" async></script></div></div><!--!--><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen  order-3 is-sticky"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="level is-mobile" href="#一、Linux-网络发送过程总览"><span class="level-left"><span class="level-item">1</span><span class="level-item">一、Linux 网络发送过程总览</span></span></a></li><li><a class="level is-mobile" href="#二、网卡启动准备"><span class="level-left"><span class="level-item">2</span><span class="level-item">二、网卡启动准备</span></span></a></li><li><a class="level is-mobile" href="#三、accept-创建新-socket"><span class="level-left"><span class="level-item">3</span><span class="level-item">三、accept 创建新 socket</span></span></a></li><li><a class="level is-mobile" href="#四、发送数据真正开始"><span class="level-left"><span class="level-item">4</span><span class="level-item">四、发送数据真正开始</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#4-1-send-系统调用实现"><span class="level-left"><span class="level-item">4.1</span><span class="level-item">4.1 send 系统调用实现</span></span></a></li><li><a class="level is-mobile" href="#4-2-传输层处理"><span class="level-left"><span class="level-item">4.2</span><span class="level-item">4.2 传输层处理</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#1）传输层拷贝"><span class="level-left"><span class="level-item">4.2.1</span><span class="level-item">1）传输层拷贝</span></span></a></li><li><a class="level is-mobile" href="#2）传输层发送"><span class="level-left"><span class="level-item">4.2.2</span><span class="level-item">2）传输层发送</span></span></a></li></ul></li><li><a class="level is-mobile" href="#4-3-网络层发送处理"><span class="level-left"><span class="level-item">4.3</span><span class="level-item">4.3 网络层发送处理</span></span></a></li><li><a class="level is-mobile" href="#4-4-邻居子系统"><span class="level-left"><span class="level-item">4.4</span><span class="level-item">4.4 邻居子系统</span></span></a></li><li><a class="level is-mobile" href="#4-5-网络设备子系统"><span class="level-left"><span class="level-item">4.5</span><span class="level-item">4.5 网络设备子系统</span></span></a></li><li><a class="level is-mobile" href="#4-6-软中断调度"><span class="level-left"><span class="level-item">4.6</span><span class="level-item">4.6 软中断调度</span></span></a></li><li><a class="level is-mobile" href="#4-7-igb-网卡驱动发送"><span class="level-left"><span class="level-item">4.7</span><span class="level-item">4.7 igb 网卡驱动发送</span></span></a></li><li><a class="level is-mobile" href="#4-8-发送完成硬中断"><span class="level-left"><span class="level-item">4.8</span><span class="level-item">4.8 发送完成硬中断</span></span></a></li></ul></li><li><a class="level is-mobile" href="#最后"><span class="level-left"><span class="level-item">5</span><span class="level-item">最后</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.jpg"></figure><p class="is-size-6 is-block">公众号：面向问题编程</p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">36</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">11</p></a></div></div></nav></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/%E4%B8%80%E6%96%87%E4%BA%86%E8%A7%A3/"><span class="level-start"><span class="level-item">一文了解</span></span><span class="level-end"><span class="level-item tag">15</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84/"><span class="level-start"><span class="level-item">技术架构</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/%E7%BB%BC%E8%BF%B0/"><span class="level-start"><span class="level-item">综述</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E9%97%AE%E9%A2%98/"><span class="level-start"><span class="level-item">问题</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="面向问题编程" height="28"></a><p class="is-size-7"><span>&copy; 2022 面向问题编程</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/lg-zoom@1.3.0/dist/lg-zoom.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>